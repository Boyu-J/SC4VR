{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb18004e-bc94-41b1-9edd-479608dd5116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = 'path/to/video_folder/' # video_folder contains video.mp4\n",
    "from collections import Counter\n",
    "from torchvision.io.video import read_video\n",
    "from torchvision.models.video import r3d_18 , R3D_18_Weights\n",
    "import cv2\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "\n",
    "from torchlars import LARS\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import av\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score, silhouette_score, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, average_precision_score, balanced_accuracy_score\n",
    "\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ce2535-ce47-4c08-8e53-164fbb3a1d4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed value\n",
    "seed = 7\n",
    "set_seed(seed)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34aeff9d-10c6-4dc3-b0ad-2971f386ab4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 cuda:1\n"
     ]
    }
   ],
   "source": [
    "device0 = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device1 = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device0, device1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e09320-f1f2-4824-8621-bbde6dcab183",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ff2db90-d2a2-4769-b4a0-187f4d177dee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the weights\n",
    "weights = R3D_18_Weights.DEFAULT\n",
    "#model = r3d_18(weights=weights)\n",
    "model = r3d_18()\n",
    "# model.load_state_dict(torch.load('r3d_18-b3b3357e.pth', weights_only=True))\n",
    "\n",
    "#import torch.nn.init as init\n",
    "\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 3)\n",
    "#init.kaiming_uniform_(model.fc.weight, nonlinearity='relu')  # Initialize weights\n",
    "#model.fc.bias.data.fill_(0)  # Initialize biases to zero\n",
    "\n",
    "# require gradient for training\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Set the model to eval mode and move to desired device.\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f81ed7-98cd-47e8-becf-740ac9fff4f8",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61539786-29ca-47e5-a4eb-b3d267364970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load labels\n",
    "events = pd.read_csv('EventType.csv')\n",
    "# events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b8054dc-5ec2-4557-ba58-04ace6eebe49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EVENTSEVERITY1\n",
       "Baseline      7860\n",
       "Near-Crash    6174\n",
       "Crash         1686\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events['EVENTSEVERITY1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e69226e2-6255-4b54-8ac2-56b4dad885e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15720"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names = os.listdir(path)\n",
    "len(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "669530d1-ec20-4479-b7eb-5737591a329d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# Set the seed\n",
    "random.seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# Your code that generates random values\n",
    "# label = pd.get_dummies(events, columns=['EVENTSEVERITY1'])\n",
    "# label = label.drop(columns=['EVENT_ID']).astype(int)\n",
    "label = events['EVENTSEVERITY1'].tolist()\n",
    "# Create a mapping from characters to integers\n",
    "#y = {char: idx for idx, char in enumerate(set(label))}\n",
    "y = {'Crash': 2, 'Baseline': 0, 'Near-Crash': 1}\n",
    "\n",
    "# Convert character elements to integers\n",
    "label = [y[char] for char in label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00abf4a7-943d-4c97-9291-d7c9a2507147",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 7860, 1: 6174, 2: 1686})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e088408-4ce5-4284-a13e-c567a9c5f087",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15720"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del events\n",
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9311dc18-9afe-4250-ae0c-55714d4d4960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the inference transforms\n",
    "weights = R3D_18_Weights.DEFAULT\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21070c1c-df54-4c99-910a-27717bf87318",
   "metadata": {},
   "source": [
    "R3D_18_Weights.KINETICS400_V1.transforms() perform the following preprocessing operations: \n",
    "\n",
    "Accepts batched (B, T, C, H, W) and single (T, C, H, W) video frame torch.Tensor objects. \n",
    "\n",
    "The frames are resized to resize_size=[128, 171] using interpolation=InterpolationMode.BILINEAR, followed by a central crop of crop_size=[112, 112]. \n",
    "\n",
    "The values are first rescaled to [0.0, 1.0] and then normalized using mean=[0.43216, 0.394666, 0.37645] and std=[0.22803, 0.22145, 0.216989]. \n",
    "\n",
    "The output dimensions are permuted to (..., C, T, H, W) tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f8ad179-c833-4a3d-bc85-1bdd6bcdc931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_video(file):\n",
    "    vid, _, _ = read_video(path + file, output_format=\"TCHW\", pts_unit='sec')\n",
    "    if vid.size(0) == 0:  # Check if the first dimension (time) is zero\n",
    "        return torch.ones(3, 76, 112, 112)  # Return a zero tensor with the specified shape\n",
    "    return preprocess(vid[:76]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de1a0c54-c962-4415-943b-4f1365f7ea44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 3, 77, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "# torch.save(X, '/home/boyuj/VTTI_Boyu/Working/Boyu/SHRP 2 video/All_video_tensor_ResNet3D_18.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a43a6b-103d-4b8f-bec1-be6bf28fba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = torch.load('/home/boyuj/VTTI_Boyu/Working/Boyu/SHRP 2 video/All_video_tensor_ResNet3D_18.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a86b9cf-7c6e-4df3-bd96-788183652d81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gives easier dataset managment and creates mini batches\n",
    "'''from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c942b7e-d49b-45c4-b4e5-ef95ce46fa6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "train_files, temp_files, train_labels, temp_labels = train_test_split(file_names, label, test_size=0.3, \n",
    "                                                                      #stratify=label, \n",
    "                                                                      random_state=7)\n",
    "\n",
    "val_files, test_files, val_labels, test_labels = train_test_split(temp_files, temp_labels, test_size=0.667, \n",
    "                                                                  #stratify=temp_labels, \n",
    "                                                                  random_state=7)\n",
    "\n",
    "# train_files, train_labels for training\n",
    "# val_files, val_labels for validation\n",
    "# test_files, test_labels for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14d2060b-13ac-49f0-9b59-42ba12113fa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11004 Counter({0: 5485, 1: 4337, 2: 1182})\n",
      "1570 Counter({0: 802, 1: 609, 2: 159})\n",
      "3146 Counter({0: 1573, 1: 1228, 2: 345})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(len(train_labels), Counter(train_labels))\n",
    "print(len(val_labels), Counter(val_labels))\n",
    "print(len(test_labels), Counter(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9eaf4f0e-3e74-490a-99ab-31e29f2491c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split training set for DataLoader\n",
    "num_splits = 4\n",
    "train_files_split = [train_files[i::num_splits] for i in range(num_splits)]\n",
    "train_labels_split = [train_labels[i::num_splits] for i in range(num_splits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dac055d-bec8-40b4-9987-4be2a61e7dbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load validation sets\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    X_val = list(executor.map(process_video, val_files))\n",
    "\n",
    "X_val = torch.stack(X_val)#.to(device1)\n",
    "Y_val = torch.as_tensor(val_labels)#.to(device1)\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Create a DataLoader for batching\n",
    "val_dataset = TensorDataset(X_val, Y_val)\n",
    "del X_val, Y_val\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)  # Adjust batch_size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5620bcc4-aff9-4347-b6d6-aaaa29d4c777",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b230a920-3a6e-433f-84af-d959fd0f6131",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4552782b-5b1f-48ff-acff-b2fdcc9c6b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1) # 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71583fe9-cc90-41ef-beb6-9b278d8b9da8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 402.6621, Val Loss: 148.8505, Val Accuracy: 0.511\n",
      "New best model saved with val accuracy: 0.510828025477707\n",
      "Epoch [2/100], Training Loss: 329.6675, Val Loss: 150.3648, Val Accuracy: 0.511\n",
      "Epoch [3/100], Training Loss: 330.1610, Val Loss: 149.8094, Val Accuracy: 0.388\n",
      "Epoch [4/100], Training Loss: 330.2533, Val Loss: 148.0592, Val Accuracy: 0.511\n",
      "Epoch [5/100], Training Loss: 329.3377, Val Loss: 148.9544, Val Accuracy: 0.511\n"
     ]
    }
   ],
   "source": [
    "results_file = open('/home/boyuj/VTTI_Boyu/Working/Boyu/SHRP 2 video/Output/Supervised L/results_SL_ScratchLearn.txt', 'a')\n",
    "num_epochs = 100\n",
    "model = model.to(device1)\n",
    "model.train()\n",
    "BatchSize = 32 # 32 is feasible\n",
    "best_val_accuracy = 0 #\n",
    "#best_val_ss = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #correct = 0  # Initialize correct predictions counter\n",
    "    #total = 0    # Initialize total predictions counter\n",
    "    running_loss = 0\n",
    "    for split in range(num_splits):\n",
    "        # Process each split\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            X_split = list(executor.map(process_video, train_files_split[split]))\n",
    "            \n",
    "        X_split = torch.stack(X_split)\n",
    "        Y_split = torch.as_tensor(train_labels_split[split])\n",
    "    \n",
    "        train_dataset = TensorDataset(X_split, Y_split)\n",
    "        del X_split, Y_split\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=BatchSize, shuffle=True)\n",
    "        del train_dataset\n",
    "   \n",
    "        for data, targets in train_loader:\n",
    "            # Get data to cuda\n",
    "            data = data.to(device1) #.float()\n",
    "            targets = targets.to(device1)\n",
    "\n",
    "            # forward\n",
    "            scores = model(data) # !!!no softmax needed! Already in the CrossEntropyLoss\n",
    "            loss = criterion(scores, targets) ## can be changed\n",
    "            running_loss += loss\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # gradient descent or adam step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            #_, predicted = torch.max(scores.data, 1)\n",
    "            #total += targets.size(0)\n",
    "            #correct += (predicted == targets).sum().item()\n",
    "            \n",
    "        \n",
    "        del train_loader, data, targets\n",
    "\n",
    "    #accuracy = correct / total  # Calculate accuracy\n",
    "    \n",
    "    # Validation phase\n",
    "    total_val_correct = 0\n",
    "    total_val = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_Y in val_loader:\n",
    "            batch_X = batch_X.to(device1)\n",
    "            batch_Y = batch_Y.to(device1)\n",
    "            val_scores = model(batch_X).softmax(-1)  # Forward pass on validation set\n",
    "            _, val_predicted = torch.max(val_scores.data, 1)  # Get predictions\n",
    "            total_val_correct += (val_predicted == batch_Y).sum().item()  # Count correct predictions\n",
    "            total_val += batch_Y.size(0)  # Total predictions\n",
    "            val_loss += criterion(model(batch_X), batch_Y)\n",
    "\n",
    "    val_accuracy = total_val_correct / total_val  # Calculate validation accuracy\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss.item():.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.3f}')\n",
    "    results_file.write(\n",
    "        f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss.item():.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.3f}\\n')\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), f'/home/boyuj/VTTI_Boyu/Working/Boyu/SHRP 2 video/Output/Supervised L/best_ScratchLearn_weights.pth')  # Save the model weights\n",
    "        #torch.save(model, f'/home/boyuj/VTTI_Boyu/Working/Boyu/SHRP 2 video/Output/Supervised L/Epoch {epoch+1}.pth')  # Save the entire model\n",
    "        print(f'New best model saved with val accuracy: {best_val_accuracy}')\n",
    "        results_file.write(f'New best model saved with val accuracy: {best_val_accuracy}\\n')\n",
    "        \n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "results_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26628114-f87f-423e-9625-14be5dff24f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a84d25e0-178c-467f-ac55-40fd563b3595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Free up unused memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba7110-22a4-443c-a161-eb5d5cb55bf4",
   "metadata": {},
   "source": [
    "On validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "576dde18-5297-4a5e-8abc-b99d69584f2e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the quality of learned latent representations\n",
    "model_test = r3d_18()\n",
    "model_test.fc = torch.nn.Linear(model_test.fc.in_features, 3)\n",
    "model_test.load_state_dict(torch.load(f'/home/boyuj/VTTI_Boyu/Working/Boyu/SHRP 2 video/Output/Supervised L/best_ScratchLearn_weights.pth', weights_only=True))\n",
    "\n",
    "model_test = torch.nn.Sequential(*(list(model_test.children())[:-1]))\n",
    "\n",
    "model_test = model_test.to(device1)\n",
    "model_test.eval()\n",
    "type(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c18ce97-7ffa-4e1b-af41-18adc4203ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.000, Val Silht score: 0.269\n"
     ]
    }
   ],
   "source": [
    "ss_result = []\n",
    "#val_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_Y in val_loader:\n",
    "        batch_X = batch_X.to(device1)\n",
    "        batch_Y = batch_Y.to(device1)\n",
    "        h_val = model_test(batch_X)\n",
    "        h_val = h_val.view(h_val.size(0), -1)\n",
    "        #val_loss += ContrastLoss(h_val, batch_Y)\n",
    "        ss = silhouette_score(h_val.cpu().numpy(), batch_Y.cpu().numpy())\n",
    "        #print(ss)\n",
    "        ss_result.append(ss)\n",
    "        #val_scores = fc_layer(h_val).softmax(-1)  # Forward pass on validation set\n",
    "        #_, val_predicted = torch.max(val_scores.data, 1)  # Get predictions\n",
    "        #total_val_correct += (val_predicted == batch_Y).sum().item()  # Count correct predictions\n",
    "        #total_val += batch_Y.size(0)  # Total predictions\n",
    "\n",
    "        #val_accuracy = total_val_correct / total_val  # Calculate validation accuracy\n",
    "mean_ss = sum(ss_result) / len(ss_result)\n",
    "#print(f'Epoch [{epoch+1}/{num_epochs}], Val loss: {val_loss:.3f}, Val Silht score: {mean_ss:.3f}')\n",
    "print(f'Val loss: {val_loss:.3f}, Val Silht score: {mean_ss:.3f}')\n",
    "        #results_file.write(f'Epoch [{epoch+1}/{num_epochs}], Val loss: {val_loss:.3f}, Val Silht score: {mean_ss:.3f}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd96d6-54c8-44b9-ae01-2af364c51ec4",
   "metadata": {},
   "source": [
    "on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "090ed8f3-ab00-477c-b8df-fd9e967af131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load test sets\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    X_test = list(executor.map(process_video, test_files))\n",
    "\n",
    "X_test = torch.stack(X_test)#.to(device1)\n",
    "Y_test = torch.as_tensor(test_labels)#.to(device1)\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Create a DataLoader for batching\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "del X_test, Y_test\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)  # Adjust batch_size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83d5004a-16e8-4b0f-979d-56bcdc808719",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test = r3d_18()\n",
    "model_test.fc = torch.nn.Linear(model_test.fc.in_features, 3)\n",
    "model_test.load_state_dict(torch.load(f'/home/boyuj/VTTI_Boyu/Working/Boyu/SHRP 2 video/Output/Supervised L/best_ScratchLearn_weights.pth', weights_only=True))\n",
    "\n",
    "model_test = torch.nn.Sequential(*(list(model_test.children())[:-1]))\n",
    "\n",
    "model_test = model_test.to(device1)\n",
    "model_test.eval()\n",
    "type(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9139941b-e6c1-4160-9252-26116e2dc669",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.000, Test Silht score: 0.242\n"
     ]
    }
   ],
   "source": [
    "ss_result = []\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_Y in test_loader:\n",
    "        batch_X = batch_X.to(device1)\n",
    "        batch_Y = batch_Y.to(device1)\n",
    "        h_test = model_test(batch_X)\n",
    "        h_test = h_test.view(h_test.size(0), -1)\n",
    "        #test_loss += ContrastLoss(h_test, batch_Y)\n",
    "        ss = silhouette_score(h_test.cpu().numpy(), batch_Y.cpu().numpy())\n",
    "        ss_result.append(ss)\n",
    "        \n",
    "mean_ss = sum(ss_result) / len(ss_result)\n",
    "print(f'Test loss: {test_loss:.3f}, Test Silht score: {mean_ss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f9944-882c-425f-aac4-1278c5408de3",
   "metadata": {},
   "source": [
    "On test set, t-SNE plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4852e9d6-9a5c-485e-b3e3-1263156737d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trained_encoder = r3d_18()\n",
    "trained_encoder.fc = torch.nn.Linear(trained_encoder.fc.in_features, 3)\n",
    "trained_encoder.load_state_dict(torch.load(f'/home/boyuj/VTTI_Boyu/Working/Boyu/SHRP 2 video/Output/Supervised L/best_ScratchLearn_weights.pth', weights_only=True))\n",
    "trained_encoder = torch.nn.Sequential(*(list(trained_encoder.children())[:-1]))\n",
    "trained_encoder = trained_encoder.to(device1)\n",
    "\n",
    "h_result = []\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_Y in test_loader:\n",
    "        batch_X = batch_X.to(device1)\n",
    "        #batch_Y = batch_Y.to(device1)\n",
    "        h_test = trained_encoder(batch_X)\n",
    "        h_test = h_test.view(h_test.size(0), -1)\n",
    "        h_result.append(h_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "53ffeb0b-4be1-4f87-9dcf-254fe0760d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unpack_tensors(tensor_list):\n",
    "    unpacked_tensors = []\n",
    "    for tensor in tensor_list:\n",
    "        for element in tensor:\n",
    "            unpacked_tensors.append(element.cpu().numpy())\n",
    "    return np.array(unpacked_tensors)\n",
    "\n",
    "h_result = unpack_tensors(h_result)\n",
    "tSNE_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cb56a52f-af74-4397-89d4-ec57a5a395bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## t-SNE plot\n",
    "# Fit t-SNE \n",
    "random.seed(7)\n",
    "\n",
    "perplexities = [5, 10, 20, 30, 40, 50]\n",
    "learning_rates = [10, 50, 100, 250, 500, 1000]\n",
    "\n",
    "# Create a mask for each category\n",
    "mask0 = tSNE_labels == 0 #'Baseline'\n",
    "mask1 = tSNE_labels == 1 #'Near-Crash'\n",
    "mask2 = tSNE_labels == 2 #'Crash'\n",
    "\n",
    "for i in perplexities:\n",
    "    for j in learning_rates:\n",
    "        tsne = TSNE(n_components=2, perplexity=i, learning_rate=j)\n",
    "        X_tsne = tsne.fit_transform(h_result)\n",
    "        # Create separate scatter plots for each category\n",
    "        plt.scatter(X_tsne[mask0, 0], X_tsne[mask0, 1], c='orange', label='Baseline', alpha=0.5, marker='o')\n",
    "        plt.scatter(X_tsne[mask1, 0], X_tsne[mask1, 1], c='green', label='Near-Crash', alpha=0.5, marker='^')\n",
    "        plt.scatter(X_tsne[mask2, 0], X_tsne[mask2, 1], c='red', label='Crash', alpha=0.5, marker='^')\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel('t-SNE 1')\n",
    "        plt.ylabel('t-SNE 2')\n",
    "        plt.title(f'Supervised learning_CE loss')\n",
    "        plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'/home/boyuj/VTTI_Boyu/Working/Boyu/SHRP 2 video/Output/Supervised L/t-SNE plot/Perplexity_{i}_LearningRate_{j}.jpg')\n",
    "        #plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c1b719-4c49-48f2-be57-d342135c3f03",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7797a55-5c8a-46e3-a500-0e40b97b7350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = R3D_18_Weights.DEFAULT\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a436fd1d-756b-4e28-b041-b0c5ec1cd895",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del X_test,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df4d123f-8cc6-4ea9-a409-ef21218b0ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load test set\n",
    "# test_files, test_labels for testing\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    X_test = list(executor.map(process_video, test_files)) #val_files))\n",
    "\n",
    "X_test = torch.stack(X_test)#.to(device1)\n",
    "Y_test = torch.as_tensor(test_labels)#.to(device1) #val_labels).to(device1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "069d0d50-514b-4254-ab3b-3a727aac4f0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DataLoader for batching\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)  # Adjust batch_size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "beb2a364-7114-470d-8eac-fac6e860de30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del batch_X, batch_Y\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afdefcf0-ba8b-4657-a57b-be4450500fca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load best model to GPU\n",
    "\n",
    "model_test = r3d_18()\n",
    "model_test.fc = torch.nn.Linear(model_test.fc.in_features, 3)\n",
    "model_test.load_state_dict(torch.load(f'/home/boyuj/VTTI_Boyu/Working/Boyu/SHRP 2 video/Output/Supervised L/best_ScratchLearn_weights.pth', weights_only=True))\n",
    "\n",
    "# model_test = torch.load(f'/home/boyuj/VTTI_Boyu/Working/Boyu/SHRP 2 video/Output/Supervised L/Epoch 27.pth',weights_only=False)  # Load the entire model\n",
    "\n",
    "model_test = model_test.to(device1)\n",
    "model_test.eval()\n",
    "type(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4bb23731-1587-4678-8538-0034f50e0b11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_test_correct = 0\n",
    "total_test = 0\n",
    "pred_score = []\n",
    "pred_label = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_Y in test_loader:\n",
    "        batch_X = batch_X.to(device1)\n",
    "        batch_Y = batch_Y.to(device1)\n",
    "        test_scores = model_test(batch_X).softmax(-1)  # Forward pass on test set\n",
    "        _, test_predicted = torch.max(test_scores.data, 1)  # Get predictions\n",
    "        total_test_correct += (test_predicted == batch_Y).sum().item()  # Count correct predictions\n",
    "        total_test += batch_Y.size(0)  # Total predictions\n",
    "        pred_score.append(test_scores)\n",
    "        pred_label.append(test_predicted)\n",
    "        \n",
    "\n",
    "test_accuracy = total_test_correct / total_test  # Calculate test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97ab5ff4-1d9d-453b-ad90-e39c5ab72336",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7930705657978385"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "78765179-75a2-47f4-96a3-1bc1b148659a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unpack_tensors(tensor_list):\n",
    "    unpacked_tensors = []\n",
    "    for tensor in tensor_list:\n",
    "        for element in tensor:\n",
    "            unpacked_tensors.append(element.cpu().numpy())\n",
    "    return np.array(unpacked_tensors)\n",
    "\n",
    "y_score = unpack_tensors(pred_score)\n",
    "y_label = unpack_tensors(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3036a2ba-e582-49fb-b475-96631801e71e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7930705657978385\n",
      "Mean Average Precision: 0.7715611990371394\n",
      "one-vs-one AUC: 0.883597294615564\n",
      "Balanced Accuracy: 0.6800913470848196\n",
      "macro Precision: 0.7301562322883474\n",
      "macro F1: 0.6953502031443901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1376,  159,   38],\n",
       "       [ 182,  997,   49],\n",
       "       [  68,  155,  122]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Accuracy\n",
    "accuracy = accuracy_score(np.array(test_labels), y_label)#test_y_true\n",
    "# Compute Balanced Accuracy\n",
    "balanced_accuracy = balanced_accuracy_score(np.array(test_labels), y_label)\n",
    "# Compute Precision\n",
    "precision = precision_score(np.array(test_labels), y_label, average='macro')\n",
    "# Compute Recall\n",
    "#recall = recall_score(np.array(test_labels), y_label, average='macro')\n",
    "# Compute F1 Score\n",
    "f1 = f1_score(np.array(test_labels), y_label, average='macro')#test_y_true\n",
    "# Compute AUC\n",
    "auc = roc_auc_score(np.array(test_labels), y_score, multi_class='ovo')\n",
    "# Compute Confusion Matrix\n",
    "cm = confusion_matrix(np.array(test_labels), y_label)#test_y_true\n",
    "# Compute Mean Average Precision\n",
    "mAP = average_precision_score(np.array(test_labels), y_score, average='macro')\n",
    "\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Mean Average Precision:\", mAP)\n",
    "print(\"one-vs-one AUC:\", auc)\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy)\n",
    "print(\"macro Precision:\", precision)\n",
    "#print(\"macro Recall:\", recall)\n",
    "print(\"macro F1:\", f1)\n",
    "\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b88aaaa8-aa6b-48f7-8d41-94ec5c7c582e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T18:40:49.020073Z",
     "iopub.status.busy": "2025-01-31T18:40:49.019559Z",
     "iopub.status.idle": "2025-01-31T18:40:49.022936Z",
     "shell.execute_reply": "2025-01-31T18:40:49.022509Z",
     "shell.execute_reply.started": "2025-01-31T18:40:49.020058Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 cuda:0\n"
     ]
    }
   ],
   "source": [
    "device0 = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device1 = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device0, device1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc58460b-4b6d-4aea-ae01-81e1446b2a2e",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add85ecb-afec-4729-83eb-8ebc66782be9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T04:28:25.384426Z",
     "iopub.status.busy": "2025-01-31T04:28:25.383935Z",
     "iopub.status.idle": "2025-01-31T04:28:25.706915Z",
     "shell.execute_reply": "2025-01-31T04:28:25.706463Z",
     "shell.execute_reply.started": "2025-01-31T04:28:25.384412Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the weights\n",
    "weights = R3D_18_Weights.DEFAULT\n",
    "#model = r3d_18(weights=weights)\n",
    "model = r3d_18()\n",
    "# model.load_state_dict(torch.load('r3d_18-b3b3357e.pth', weights_only=True))\n",
    "\n",
    "#import torch.nn.init as init\n",
    "\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 3)\n",
    "#init.kaiming_uniform_(model.fc.weight, nonlinearity='relu')  # Initialize weights\n",
    "#model.fc.bias.data.fill_(0)  # Initialize biases to zero\n",
    "\n",
    "# require gradient for training\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Set the model to eval mode and move to desired device.\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf19d6-2fe9-4695-97a3-f6c073b870d2",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "909b62b8-5e0e-4668-bd24-787285d4446b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T18:50:55.052083Z",
     "iopub.status.busy": "2025-01-31T18:50:55.051291Z",
     "iopub.status.idle": "2025-01-31T18:50:57.225302Z",
     "shell.execute_reply": "2025-01-31T18:50:57.224858Z",
     "shell.execute_reply.started": "2025-01-31T18:50:55.052068Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15720"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load labels\n",
    "events = pd.read_csv('EventType.csv')\n",
    "path = 'Original_video/'\n",
    "file_names = os.listdir(path)\n",
    "len(file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaa3ffb6-99da-4cc4-84ce-39f277cc9108",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T18:50:57.233040Z",
     "iopub.status.busy": "2025-01-31T18:50:57.232910Z",
     "iopub.status.idle": "2025-01-31T18:50:57.236811Z",
     "shell.execute_reply": "2025-01-31T18:50:57.236305Z",
     "shell.execute_reply.started": "2025-01-31T18:50:57.233029Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# Set the seed\n",
    "random.seed(7)\n",
    "np.random.seed(7)\n",
    "\n",
    "# Your code that generates random values\n",
    "# label = pd.get_dummies(events, columns=['EVENTSEVERITY1'])\n",
    "# label = label.drop(columns=['EVENT_ID']).astype(int)\n",
    "label = events['EVENTSEVERITY1'].tolist()\n",
    "# Create a mapping from characters to integers\n",
    "#y = {char: idx for idx, char in enumerate(set(label))}\n",
    "y = {'Crash': 2, 'Baseline': 0, 'Near-Crash': 1}\n",
    "\n",
    "# Convert character elements to integers\n",
    "label = [y[char] for char in label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73528873-8cc7-4c54-a424-5a8a3ad23a2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T18:50:59.005322Z",
     "iopub.status.busy": "2025-01-31T18:50:59.005170Z",
     "iopub.status.idle": "2025-01-31T18:50:59.007783Z",
     "shell.execute_reply": "2025-01-31T18:50:59.007376Z",
     "shell.execute_reply.started": "2025-01-31T18:50:59.005311Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the inference transforms\n",
    "weights = R3D_18_Weights.DEFAULT\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e318f22c-69c9-40b6-a307-09f7c7c79908",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T18:51:00.574379Z",
     "iopub.status.busy": "2025-01-31T18:51:00.573919Z",
     "iopub.status.idle": "2025-01-31T18:51:00.576978Z",
     "shell.execute_reply": "2025-01-31T18:51:00.576511Z",
     "shell.execute_reply.started": "2025-01-31T18:51:00.574366Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_video(file):\n",
    "    vid, _, _ = read_video(path + file, output_format=\"TCHW\", pts_unit='sec')\n",
    "    if vid.size(0) == 0:  # Check if the first dimension (time) is zero\n",
    "        return torch.ones(3, 77, 112, 112)  # Return a zero tensor with the specified shape\n",
    "    return preprocess(vid[:77]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2f76915-0b30-4c6a-ae84-41d4dd65b91e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T18:51:02.155094Z",
     "iopub.status.busy": "2025-01-31T18:51:02.154959Z",
     "iopub.status.idle": "2025-01-31T18:51:02.162853Z",
     "shell.execute_reply": "2025-01-31T18:51:02.162457Z",
     "shell.execute_reply.started": "2025-01-31T18:51:02.155083Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "train_files, temp_files, train_labels, temp_labels = train_test_split(file_names, label, test_size=0.3, \n",
    "                                                                      #stratify=label, \n",
    "                                                                      random_state=7)\n",
    "\n",
    "val_files, test_files, val_labels, test_labels = train_test_split(temp_files, temp_labels, test_size=0.667, \n",
    "                                                                  #stratify=temp_labels, \n",
    "                                                                  random_state=7)\n",
    "\n",
    "# train_files, train_labels for training\n",
    "# val_files, val_labels for validation\n",
    "# test_files, test_labels for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ab7e9b7-cd2b-4bc4-9d01-909fa5655b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import pickle\n",
    "with open('train_X.pkl', 'wb') as f:\n",
    "    pickle.dump(train_files, f)\n",
    "with open('train_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(train_labels, f)\n",
    "\n",
    "with open('val_X.pkl', 'wb') as f:\n",
    "    pickle.dump(val_files, f)\n",
    "with open('val_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(val_labels, f)\n",
    "\n",
    "with open('test_X.pkl', 'wb') as f:\n",
    "    pickle.dump(test_files, f)\n",
    "with open('test_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(test_labels, f)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84341833-3f36-499b-a1df-adb54ad9f466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T04:28:38.819778Z",
     "iopub.status.busy": "2025-01-31T04:28:38.819299Z",
     "iopub.status.idle": "2025-01-31T04:28:38.822225Z",
     "shell.execute_reply": "2025-01-31T04:28:38.821818Z",
     "shell.execute_reply.started": "2025-01-31T04:28:38.819764Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split training set for DataLoader\n",
    "num_splits = 4\n",
    "train_files_split = [train_files[i::num_splits] for i in range(num_splits)]\n",
    "train_labels_split = [train_labels[i::num_splits] for i in range(num_splits)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a478ab9b-a5e3-448f-bf62-2ca261b95fd3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load validation sets\n",
    "import av\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    X_val = list(executor.map(process_video, val_files))\n",
    "\n",
    "X_val = torch.stack(X_val)#.to(device1)\n",
    "Y_val = torch.as_tensor(val_labels)#.to(device1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cea7dd7f-e717-4612-91c9-17a0c4e0955b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#torch.save(X_val, 'Processed_video_r3d18/X_val_frame77.pt')\n",
    "#torch.save(Y_val, 'Processed_video_r3d18/Y_val_frame77.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "539e9da2-4c27-47cb-a8a1-3681db430707",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T04:28:41.985542Z",
     "iopub.status.busy": "2025-01-31T04:28:41.985067Z",
     "iopub.status.idle": "2025-01-31T04:29:23.055939Z",
     "shell.execute_reply": "2025-01-31T04:29:23.055318Z",
     "shell.execute_reply.started": "2025-01-31T04:28:41.985529Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_val = torch.load('Processed_video_r3d18/X_val_frame77.pt', weights_only=True)\n",
    "Y_val = torch.load('Processed_video_r3d18/Y_val_frame77.pt', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb557021-400c-4d1c-a7cc-410f8dd1a413",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T04:29:23.056975Z",
     "iopub.status.busy": "2025-01-31T04:29:23.056825Z",
     "iopub.status.idle": "2025-01-31T04:29:23.059916Z",
     "shell.execute_reply": "2025-01-31T04:29:23.059509Z",
     "shell.execute_reply.started": "2025-01-31T04:29:23.056962Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Create a DataLoader for batching\n",
    "val_dataset = TensorDataset(X_val, Y_val)\n",
    "del X_val, Y_val\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)  # Adjust batch_size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d8af56d-62fd-4d64-991f-494a9712a055",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T04:29:23.060403Z",
     "iopub.status.busy": "2025-01-31T04:29:23.060289Z",
     "iopub.status.idle": "2025-01-31T04:29:23.063016Z",
     "shell.execute_reply": "2025-01-31T04:29:23.062714Z",
     "shell.execute_reply.started": "2025-01-31T04:29:23.060393Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c764a9a0-7efc-4d3b-bacf-c8e55064b965",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "889ff02d-143a-42e4-9321-3dc4820d8157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "for split in range(num_splits):\n",
    "    # Process each split\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        X_split = list(executor.map(process_video, train_files_split[split]))\n",
    "\n",
    "    X_split = torch.stack(X_split)\n",
    "    Y_split = torch.as_tensor(train_labels_split[split])\n",
    "    torch.save(X_split, f'Processed_video_r3d18/train/X_train_frame77_{split}.pt')\n",
    "    torch.save(Y_split, f'Processed_video_r3d18/train/Y_train_frame77_{split}.pt')\n",
    "    del X_split, Y_split\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1e0aa60-ff9c-4937-a3ec-f7b4c4e47eae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T04:25:51.520865Z",
     "iopub.status.busy": "2025-01-31T04:25:51.520588Z",
     "iopub.status.idle": "2025-01-31T04:25:51.526327Z",
     "shell.execute_reply": "2025-01-31T04:25:51.525994Z",
     "shell.execute_reply.started": "2025-01-31T04:25:51.520852Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66873291, 0.84574591, 3.10321489])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59204007-d1e9-409e-8497-b481c404ffbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T04:29:51.611954Z",
     "iopub.status.busy": "2025-01-31T04:29:51.611685Z",
     "iopub.status.idle": "2025-01-31T04:29:51.865143Z",
     "shell.execute_reply": "2025-01-31T04:29:51.864734Z",
     "shell.execute_reply.started": "2025-01-31T04:29:51.611942Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# in whole dataset: 0= 7860, 1= 6174, 2= 1686; 1,686 crash, 6,174 near-crash, 7,860 baseline; weight = [2.0, 2.6, 9.3]\n",
    "# in training dataset: 0: 5485, 1: 4337, 2: 1182; weight = [0.7, 0.8, 3.1]\n",
    "\n",
    "class_weights = torch.tensor([0.7, 0.8, 3.1], device=device1)\n",
    "#class_weights = torch.tensor([1.0, 1.0, 1.0], device=device1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3) # 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3826a1d-cc16-477c-95a1-6bfd29d0c2fa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "model = model.to(device1)\n",
    "model.train()\n",
    "BatchSize = 32 # 32 is feasible\n",
    "best_val_accuracy = 0.511 #\n",
    "#best_val_ss = 0\n",
    "#scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    results_file = open('Output/Supervised L_WLoss/results_SL_WLoss_ScratchLearn.txt', 'a')\n",
    "    #correct = 0  # Initialize correct predictions counter\n",
    "    #total = 0    # Initialize total predictions counter\n",
    "    running_loss = 0\n",
    "    for split in range(num_splits):\n",
    "        # Process each split\n",
    "        #with ThreadPoolExecutor() as executor:\n",
    "            #X_split = list(executor.map(process_video, train_files_split[split]))\n",
    "            \n",
    "        #X_split = torch.stack(X_split)\n",
    "        #Y_split = torch.as_tensor(train_labels_split[split])\n",
    "        \n",
    "        X_split = torch.load(f'Processed_video_r3d18/train/X_train_frame77_{split}.pt', weights_only=True)\n",
    "        Y_split = torch.load(f'Processed_video_r3d18/train/Y_train_frame77_{split}.pt', weights_only=True)\n",
    "        train_dataset = TensorDataset(X_split, Y_split)\n",
    "        del X_split, Y_split\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=BatchSize, shuffle=True)\n",
    "        del train_dataset\n",
    "   \n",
    "        for data, targets in train_loader:\n",
    "            # Get data to cuda\n",
    "            data = data.to(device1) #.float()\n",
    "            targets = targets.to(device1)\n",
    "            # forward\n",
    "            scores = model(data) # !!!no softmax needed! Already in the CrossEntropyLoss\n",
    "            loss = criterion(scores, targets) ## can be changed\n",
    "            running_loss += loss\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # gradient descent or adam step\n",
    "            optimizer.step()\n",
    "            # Calculate accuracy\n",
    "            #_, predicted = torch.max(scores.data, 1)\n",
    "            #total += targets.size(0)\n",
    "            #correct += (predicted == targets).sum().item()\n",
    "        del train_loader, data, targets\n",
    "    #accuracy = correct / total  # Calculate accuracy\n",
    "    \n",
    "    # Validation phase\n",
    "    total_val_correct = 0\n",
    "    total_val = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_Y in val_loader:\n",
    "            batch_X = batch_X.to(device1)\n",
    "            batch_Y = batch_Y.to(device1)\n",
    "            val_scores = model(batch_X).softmax(-1)  # Forward pass on validation set\n",
    "            _, val_predicted = torch.max(val_scores.data, 1)  # Get predictions\n",
    "            total_val_correct += (val_predicted == batch_Y).sum().item()  # Count correct predictions\n",
    "            total_val += batch_Y.size(0)  # Total predictions\n",
    "            val_loss += criterion(model(batch_X), batch_Y)\n",
    "\n",
    "    val_accuracy = total_val_correct / total_val  # Calculate validation accuracy\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss.item():.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.3f}')\n",
    "    results_file.write(\n",
    "        f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss.item():.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.3f}\\n')\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), f'Output/Supervised L_WLoss/best_ScratchLearn_weights.pth')  # Save the model weights\n",
    "        #torch.save(model, f'/home/boyuj/VTTI_Boyu/Working/Boyu/SHRP 2 video/Output/Supervised L/Epoch {epoch+1}.pth')  # Save the entire model\n",
    "        print(f'New best model saved with val accuracy: {best_val_accuracy}')\n",
    "        results_file.write(f'New best model saved with val accuracy: {best_val_accuracy}\\n')\n",
    "        \n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    results_file.close()\n",
    "    #scheduler.step()\n",
    "\n",
    "results_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1637c0-c610-4a00-befd-8b8be777ca23",
   "metadata": {},
   "source": [
    "on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb9f1ae9-5057-44e4-a189-dace8cb594d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test sets\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    X_test = list(executor.map(process_video, test_files))\n",
    "\n",
    "X_test = torch.stack(X_test)#.to(device1)\n",
    "Y_test = torch.as_tensor(test_labels)#.to(device1)\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Create a DataLoader for batching\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "del X_test, Y_test\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)  # Adjust batch_size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03b2a1e6-b452-4d84-ac84-35aa13cae131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test = r3d_18()\n",
    "model_test.fc = torch.nn.Linear(model_test.fc.in_features, 3)\n",
    "model_test.load_state_dict(torch.load(f'Output/Supervised L_WLoss/best_ScratchLearn_weights.pth', weights_only=True))\n",
    "\n",
    "model_test = torch.nn.Sequential(*(list(model_test.children())[:-1]))\n",
    "\n",
    "model_test = model_test.to(device1)\n",
    "model_test.eval()\n",
    "type(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "243c752a-dbba-4ef6-b1c4-5b0b20dcdfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.000, Test Silht score: 0.336, Test CH index: 10.692019894376559, Test DB index: 0.804744944225678\n"
     ]
    }
   ],
   "source": [
    "ss_result = []\n",
    "DB_index = []\n",
    "CH_index = []\n",
    "test_loss = 0\n",
    "h_result = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_Y in test_loader:\n",
    "        batch_X = batch_X.to(device1)\n",
    "        batch_Y = batch_Y.to(device1)\n",
    "        h_test = model_test(batch_X)\n",
    "        h_test = h_test.view(h_test.size(0), -1)\n",
    "        #test_loss += ContrastLoss(h_test, batch_Y)\n",
    "        ss = silhouette_score(h_test.cpu().numpy(), batch_Y.cpu().numpy())\n",
    "        ch = calinski_harabasz_score(h_test.cpu().numpy(), batch_Y.cpu().numpy())\n",
    "        db = davies_bouldin_score(h_test.cpu().numpy(), batch_Y.cpu().numpy())\n",
    "\n",
    "        ss_result.append(ss)\n",
    "        CH_index.append(ch)\n",
    "        DB_index.append(db)\n",
    "        \n",
    "mean_ss = sum(ss_result) / len(ss_result)\n",
    "mean_ch = sum(CH_index) / len(CH_index)\n",
    "mean_db = sum(DB_index) / len(DB_index)\n",
    "\n",
    "print(f'Test loss: {test_loss:.3f}, Test Silht score: {mean_ss:.3f}, Test CH index: {mean_ch}, Test DB index: {mean_db}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074bfe6d-e946-4028-ae11-9b60e94f3639",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fc59a4-0403-40d5-a94f-b6d5290a881e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_test = r3d_18()\n",
    "model_test.fc = torch.nn.Linear(model_test.fc.in_features, 3)\n",
    "model_test.load_state_dict(torch.load(f'Output/Supervised L_WLoss/best_ScratchLearn_weights.pth', weights_only=True))\n",
    "\n",
    "#model_test = torch.nn.Sequential(*(list(model_test.children())[:-1]))\n",
    "\n",
    "model_test = model_test.to(device1)\n",
    "model_test.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5c80313-ee68-4f14-a7eb-d32410332c55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T18:41:10.709206Z",
     "iopub.status.busy": "2025-01-31T18:41:10.708747Z",
     "iopub.status.idle": "2025-01-31T18:42:28.981908Z",
     "shell.execute_reply": "2025-01-31T18:42:28.980894Z",
     "shell.execute_reply.started": "2025-01-31T18:41:10.709192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = torch.load('Processed_video_r3d18/X_test_frame77.pt', weights_only=True)\n",
    "Y_test = torch.load('Processed_video_r3d18/Y_test_frame77.pt', weights_only=True)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Create a DataLoader for batching\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "del X_test, Y_test\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)  # Adjust batch_size as needed\n",
    "\n",
    "type(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "596e8311-a861-4f26-b9ae-9290a98a3bc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T18:47:15.943718Z",
     "iopub.status.busy": "2025-01-31T18:47:15.943328Z",
     "iopub.status.idle": "2025-01-31T18:47:36.403469Z",
     "shell.execute_reply": "2025-01-31T18:47:36.402893Z",
     "shell.execute_reply.started": "2025-01-31T18:47:15.943701Z"
    }
   },
   "outputs": [],
   "source": [
    "total_test_correct = 0\n",
    "total_test = 0\n",
    "pred_score = []\n",
    "pred_label = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_Y in test_loader:\n",
    "        batch_X = batch_X.to(device1)\n",
    "        batch_Y = batch_Y.to(device1)\n",
    "        test_scores = model_test(batch_X).softmax(-1)  # Forward pass on test set\n",
    "        _, test_predicted = torch.max(test_scores.data, 1)  # Get predictions\n",
    "        total_test_correct += (test_predicted == batch_Y).sum().item()  # Count correct predictions\n",
    "        total_test += batch_Y.size(0)  # Total predictions\n",
    "        pred_score.append(test_scores)\n",
    "        pred_label.append(test_predicted)\n",
    "        \n",
    "\n",
    "test_accuracy = total_test_correct / total_test  # Calculate test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71648969-7703-492f-9392-a376323956c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T18:47:45.382496Z",
     "iopub.status.busy": "2025-01-31T18:47:45.381918Z",
     "iopub.status.idle": "2025-01-31T18:47:45.475455Z",
     "shell.execute_reply": "2025-01-31T18:47:45.474993Z",
     "shell.execute_reply.started": "2025-01-31T18:47:45.382481Z"
    }
   },
   "outputs": [],
   "source": [
    "def unpack_tensors(tensor_list):\n",
    "    unpacked_tensors = []\n",
    "    for tensor in tensor_list:\n",
    "        for element in tensor:\n",
    "            unpacked_tensors.append(element.cpu().numpy())\n",
    "    return np.array(unpacked_tensors)\n",
    "\n",
    "y_score = unpack_tensors(pred_score)\n",
    "y_label = unpack_tensors(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59b8a020-b71c-4ad1-b12e-136730fa5b14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T18:51:10.591319Z",
     "iopub.status.busy": "2025-01-31T18:51:10.590630Z",
     "iopub.status.idle": "2025-01-31T18:51:10.608059Z",
     "shell.execute_reply": "2025-01-31T18:51:10.607648Z",
     "shell.execute_reply.started": "2025-01-31T18:51:10.591306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.871900826446281\n",
      "Mean Average Precision: 0.8933728005063225\n",
      "one-vs-one AUC: 0.9511113180135401\n",
      "Balanced Accuracy: 0.839929903967962\n",
      "macro Precision: 0.8299935171835077\n",
      "macro F1: 0.834456726191314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1444,   85,   44],\n",
       "       [ 124, 1038,   66],\n",
       "       [  35,   49,  261]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Accuracy\n",
    "accuracy = accuracy_score(np.array(test_labels), y_label)#test_y_true\n",
    "# Compute Balanced Accuracy\n",
    "balanced_accuracy = balanced_accuracy_score(np.array(test_labels), y_label)\n",
    "# Compute Precision\n",
    "precision = precision_score(np.array(test_labels), y_label, average='macro')\n",
    "# Compute Recall\n",
    "#recall = recall_score(np.array(test_labels), y_label, average='macro')\n",
    "# Compute F1 Score\n",
    "f1 = f1_score(np.array(test_labels), y_label, average='macro')#test_y_true\n",
    "# Compute AUC\n",
    "auc = roc_auc_score(np.array(test_labels), y_score, multi_class='ovo')\n",
    "# Compute Confusion Matrix\n",
    "cm = confusion_matrix(np.array(test_labels), y_label)#test_y_true\n",
    "# Compute Mean Average Precision\n",
    "mAP = average_precision_score(np.array(test_labels), y_score, average='macro')\n",
    "\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Mean Average Precision:\", mAP)\n",
    "print(\"one-vs-one AUC:\", auc)\n",
    "print(\"Balanced Accuracy:\", balanced_accuracy)\n",
    "print(\"macro Precision:\", precision)\n",
    "#print(\"macro Recall:\", recall)\n",
    "print(\"macro F1:\", f1)\n",
    "\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2db6120-4c7d-4a5a-a5ab-ce89648a08ec",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73ac049-10fd-413c-b695-580bcf59f569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "230c04c7-90e6-49ea-aaa4-cf665e333c5f",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46947e62-097c-4ddf-9cbb-ae9bbe410af6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "902f624a-52f4-44fc-a9c7-06eefe598bf4",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eebf41-e239-46c0-ad9d-98bcad67eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "\n",
    "# Define loss function and optimizer\n",
    "# in whole dataset: 0= 7860, 1= 6174, 2= 1686\n",
    "# 1,686 crash, 6,174 near-crash, 7,860 baseline\n",
    "# 0: 2.0, 1: 2.55, 2: 9.32\n",
    "#class_weights = torch.tensor([2.0, 2.55, 9.32], device=device1)\n",
    "#criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "criterion = sigmoid_focal_loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4) # 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8df671-0134-4414-b7d1-e1c815d6edbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1e3e9-b2f9-43e2-8b40-a0437c5d644b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f831899-9035-4cf4-a90b-59db106f8b01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1 cuda:1\n"
     ]
    }
   ],
   "source": [
    "device0 = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device1 = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device0, device1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68782dfc-40d5-4273-a293-0d4ee0feaffe",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b0a35e-ba8f-4a15-bcf0-159ee141a4ce",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the weights\n",
    "#weights = Swin3D_B_Weights.DEFAULT\n",
    "#model = r3d_18(weights=weights)\n",
    "model = swin3d_b()\n",
    "# model.load_state_dict(torch.load('r3d_18-b3b3357e.pth', weights_only=True))\n",
    "#import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "473ef8eb-332e-4217-8063-ef921858442f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformer3d(\n",
       "  (patch_embed): PatchEmbed3d(\n",
       "    (proj): Conv3d(3, 128, kernel_size=(2, 4, 4), stride=(2, 4, 4))\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (features): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.004347826086956522, mode=row)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): PatchMerging(\n",
       "      (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.013043478260869568, mode=row)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): PatchMerging(\n",
       "      (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.021739130434782608, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.030434782608695653, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0391304347826087, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.04782608695652174, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05652173913043478, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06521739130434782, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07391304347826087, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08260869565217392, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09130434782608696, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): PatchMerging(\n",
       "      (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "      (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (avgpool): AdaptiveAvgPool3d(output_size=1)\n",
       "  (head): Linear(in_features=1024, out_features=400, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3622946-e30c-475a-8ace-7410b8fcade4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.head = torch.nn.Linear(model.head.in_features, 3)\n",
    "\n",
    "model.load_state_dict(torch.load('Output/Supervised L_SwinTrans/best_ScratchLearn_weights.pth', weights_only=True))\n",
    "#model.head = torch.nn.Linear(model.head.in_features, 512)\n",
    "#model.fc = torch.nn.Linear(512, 3)\n",
    "\n",
    "#init.kaiming_uniform_(model.fc.weight, nonlinearity='relu')  # Initialize weights\n",
    "#model.fc.bias.data.fill_(0)  # Initialize biases to zero\n",
    "\n",
    "# require gradient for training\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Set the model to eval mode and move to desired device.\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61d49b01-4136-4fc9-a469-03c94c10c6bf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformer3d(\n",
       "  (patch_embed): PatchEmbed3d(\n",
       "    (proj): Conv3d(3, 128, kernel_size=(2, 4, 4), stride=(2, 4, 4))\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (features): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.004347826086956522, mode=row)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): PatchMerging(\n",
       "      (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.013043478260869568, mode=row)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): PatchMerging(\n",
       "      (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.021739130434782608, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.030434782608695653, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0391304347826087, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.04782608695652174, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05652173913043478, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06521739130434782, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07391304347826087, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08260869565217392, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09130434782608696, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): PatchMerging(\n",
       "      (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "      (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention3d(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (avgpool): AdaptiveAvgPool3d(output_size=1)\n",
       "  (head): Linear(in_features=1024, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2386195-a4fa-4ba5-ae41-f7ae9e94b9cf",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ca51f8b-b66e-4209-8aa4-5ec23bcb7038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load labels\n",
    "events = pd.read_csv('EventType.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa198e3c-2c84-42e1-9671-73ea01ab9d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15720"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names = os.listdir(path)\n",
    "len(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "560abdbd-1874-4c7f-b3a4-e64aa185eebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Your code that generates random values\n",
    "# label = pd.get_dummies(events, columns=['EVENTSEVERITY1'])\n",
    "# label = label.drop(columns=['EVENT_ID']).astype(int)\n",
    "label = events['EVENTSEVERITY1'].tolist()\n",
    "# Create a mapping from characters to integers\n",
    "#y = {char: idx for idx, char in enumerate(set(label))}\n",
    "y = {'Crash': 2, 'Baseline': 0, 'Near-Crash': 1}\n",
    "\n",
    "# Convert character elements to integers\n",
    "label = [y[char] for char in label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6019682-325c-4ba3-a1d1-8739c04bb572",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the inference transforms\n",
    "weights = Swin3D_B_Weights.DEFAULT\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_video(file):\n",
    "    vid, _, _ = read_video(path + file, output_format=\"TCHW\", pts_unit='sec')\n",
    "    if vid.size(0) == 0:  # Check if the first dimension (time) is zero\n",
    "        return torch.ones(3, 77, 224, 224)  # Return a zero tensor with the specified shape\n",
    "    return preprocess(vid[:77]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "886335dc-11eb-4490-9321-a7e8f55d4339",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "train_files, temp_files, train_labels, temp_labels = train_test_split(file_names, label, test_size=0.3, \n",
    "                                                                      #stratify=label, \n",
    "                                                                      random_state=7)\n",
    "\n",
    "val_files, test_files, val_labels, test_labels = train_test_split(temp_files, temp_labels, test_size=0.667, \n",
    "                                                                  #stratify=temp_labels, \n",
    "                                                                  random_state=7)\n",
    "\n",
    "# train_files, train_labels for training\n",
    "# val_files, val_labels for validation\n",
    "# test_files, test_labels for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01519417-c09a-4166-ac04-aec92a56db3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split training set for DataLoader\n",
    "num_splits = 8 #4\n",
    "train_files_split = [train_files[i::num_splits] for i in range(num_splits)]\n",
    "train_labels_split = [train_labels[i::num_splits] for i in range(num_splits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c27f0fdf-8b00-4008-8e35-1138d21cc054",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load validation sets\n",
    "'''from concurrent.futures import ThreadPoolExecutor\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    X_val = list(executor.map(process_video, val_files))\n",
    "\n",
    "X_val = torch.stack(X_val)#.to(device1)\n",
    "Y_val = torch.as_tensor(val_labels)#.to(device1)\n",
    "\n",
    "\n",
    "#from torch.utils.data import DataLoader, TensorDataset\n",
    "#val_dataset = TensorDataset(X_val, Y_val)\n",
    "#del X_val, Y_val\n",
    "#val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False)  # batch_size = 10\n",
    "#del val_dataset\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3440e818-3d4d-4f35-bc56-317bada9e549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#torch.save(X_val, 'Processed_video_SwinTransformer/X_val_frame77.pt')\n",
    "#torch.save(Y_val, 'Processed_video_SwinTransformer/Y_val_frame77.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "913db11c-e660-4039-a7e6-7d8d8f7243a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_val = torch.load('Processed_video_SwinTransformer/X_val_frame77.pt', weights_only=True)\n",
    "Y_val = torch.load('Processed_video_SwinTransformer/Y_val_frame77.pt', weights_only=True)\n",
    "val_dataset = TensorDataset(X_val, Y_val)\n",
    "del X_val, Y_val\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False) #, num_workers=16\n",
    "#del val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f01fd54f-9d9b-4349-aab8-4ed5f14ca48b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6219bd1b-a4b8-4f10-8c8c-5280b5c7af94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "for split in range(num_splits):\n",
    "    # Process each split\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        X_split = list(executor.map(process_video, train_files_split[split]))\n",
    "\n",
    "    X_split = torch.stack(X_split)\n",
    "    Y_split = torch.as_tensor(train_labels_split[split])\n",
    "    torch.save(X_split, f'Processed_video_SwinTransformer/train/X_train_frame77_{split}.pt')\n",
    "    torch.save(Y_split, f'Processed_video_SwinTransformer/train/Y_train_frame77_{split}.pt')\n",
    "    del X_split, Y_split\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de6388-10b4-44e0-8fc7-8f2e861f86c7",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f946747-1c21-47ed-8eb2-746c9444f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''num_splits = 8\n",
    "train_datasets = []\n",
    "\n",
    "def process_split(split):\n",
    "    X_split = torch.load(f'Processed_video_SwinTransformer/train/X_train_frame77_{split}.pt', weights_only=True)\n",
    "    Y_split = torch.load(f'Processed_video_SwinTransformer/train/Y_train_frame77_{split}.pt', weights_only=True)\n",
    "    return X_split, Y_split\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = executor.map(process_split, range(num_splits))\n",
    "\n",
    "for X_split, Y_split in results:\n",
    "    #Xs.append(X_split)\n",
    "    #Ys.append(Y_split)\n",
    "    train_datasets.append(TensorDataset(X_split, Y_split))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b48197e1-03bf-4608-9ba4-8a62bcb02841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr= 1e-3) # 1e-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4406b406-0b74-4e4b-9380-d3e49db248fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#results_file = open('/home/boyuj/VTTI_Boyu/Working/Boyu/SHRP 2 video/Output/Supervised L_SwinTrans/results_SL_SwinTrans_ScratchLearn.txt', 'a')\n",
    "num_epochs = 100\n",
    "model = model.to(device1)\n",
    "model.train()\n",
    "BatchSize = 5 # 32\n",
    "best_val_accuracy = 0.51146 #\n",
    "#best_val_ss = 0\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    results_file = open('Output/Supervised L_SwinTrans/results_SL_SwinTrans_ScratchLearn.txt', 'a')\n",
    "    #correct = 0  # Initialize correct predictions counter\n",
    "    #total = 0    # Initialize total predictions counter\n",
    "    running_loss = 0\n",
    "    for split in range(num_splits):\n",
    "        # Process each split\n",
    "        #with ThreadPoolExecutor() as executor:\n",
    "         #   X_split = list(executor.map(process_video, train_files_split[split]))\n",
    "        \n",
    "        X_split = torch.load(f'Processed_video_SwinTransformer/train/X_train_frame77_{split}.pt', weights_only=True)\n",
    "        Y_split = torch.load(f'Processed_video_SwinTransformer/train/Y_train_frame77_{split}.pt', weights_only=True)\n",
    "    \n",
    "        train_dataset = TensorDataset(X_split, Y_split)\n",
    "        del X_split, Y_split\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=BatchSize, shuffle=True)\n",
    "        del train_dataset\n",
    "   \n",
    "        for data, targets in train_loader:\n",
    "            # Get data to cuda\n",
    "            data = data.to(device1) #.float()\n",
    "            targets = targets.to(device1)\n",
    "\n",
    "            # forward\n",
    "            scores = model(data)\n",
    "            loss = criterion(scores, targets) ## can be changed\n",
    "            running_loss += loss\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # gradient descent or adam step\n",
    "            optimizer.step()\n",
    "        \n",
    "        del train_loader, data, targets\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Validation phase\n",
    "    total_val_correct = 0\n",
    "    total_val = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_Y in val_loader:\n",
    "            batch_X = batch_X.to(device1)\n",
    "            batch_Y = batch_Y.to(device1)\n",
    "            val_scores = model(batch_X).softmax(-1)  # Forward pass on validation set\n",
    "            _, val_predicted = torch.max(val_scores.data, 1)  # Get predictions\n",
    "            total_val_correct += (val_predicted == batch_Y).sum().item()  # Count correct predictions\n",
    "            total_val += batch_Y.size(0)  # Total predictions\n",
    "            val_loss += criterion(model(batch_X), batch_Y)\n",
    "            del batch_X, batch_Y\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    val_accuracy = total_val_correct / total_val  # Calculate validation accuracy\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss.item():.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.3f}')\n",
    "    results_file.write(\n",
    "        f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss.item():.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.3f}\\n')\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), f'Output/Supervised L_SwinTrans/best_ScratchLearn_weights.pth')  # Save the model weights\n",
    "        #torch.save(model, f'/home/boyuj/VTTI_Boyu/Working/Boyu/SHRP 2 video/Output/Supervised L/Epoch {epoch+1}.pth')  # Save the entire model\n",
    "        print(f'New best model saved with val accuracy: {best_val_accuracy}')\n",
    "        results_file.write(f'New best model saved with val accuracy: {best_val_accuracy}\\n')\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    results_file.close()\n",
    "    scheduler.step()\n",
    "    \n",
    "    \n",
    "results_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b1fd79-cb22-48e2-b1df-69c2477a776f",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0605733-ae85-4d71-95df-ab57693c9849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eee1f9d4-15a3-4ebe-94cb-8dab7ca19941",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 cuda:1\n"
     ]
    }
   ],
   "source": [
    "device0 = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device1 = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device0, device1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "986fc28d-2827-46a2-a02f-b9b7343c40df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the default CUDA device to GPU 0 (for example)\n",
    "torch.cuda.set_device(0)  # Change the index to the desired GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40dd394-cd4f-4377-a997-00a292c2a2fa",
   "metadata": {},
   "source": [
    "# load video encoder: Video ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc52f7b7-7603-4a50-991e-45d2ea1253c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the weights \n",
    "weights = R3D_18_Weights.DEFAULT\n",
    "\n",
    "encoder_anchor = r3d_18()\n",
    "\n",
    "# remove the classification head. Keep the backbone\n",
    "encoder_anchor = torch.nn.Sequential(*(list(encoder_anchor.children())[:-1]))\n",
    "\n",
    "# require gradient for training\n",
    "for param in encoder_anchor.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23c0355-fe8b-4ec6-ba0e-2cce3faeeddf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883a3c4d-2082-4038-a4b4-d323699f3970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "events = pd.read_csv('Example_video_label.csv') # change to your label file\n",
    "\n",
    "file_names = os.listdir(path)\n",
    "len(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63f106c6-dbb4-4fa6-b647-53dbe9ad72a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label = events['EVENTSEVERITY1'].tolist()\n",
    "y = {'Crash': 2, 'Baseline': 0, 'Near-Crash': 1}\n",
    "\n",
    "# Convert character elements to integers\n",
    "label = [y[char] for char in label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "490ad57f-ff8c-49e7-a7be-901cd9898790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the inference transforms\n",
    "weights = R3D_18_Weights.DEFAULT\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "\n",
    "def process_video(file):\n",
    "    vid, _, _ = read_video(path + file, output_format=\"TCHW\", pts_unit='sec')\n",
    "    return preprocess(vid[:77]) # extract first 77 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51581183-53ca-4eae-a5ad-a0442eba91fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_files, temp_files, train_labels, temp_labels = train_test_split(file_names, label, test_size=0.3, \n",
    "                                                                      random_state=7)\n",
    "\n",
    "val_files, test_files, val_labels, test_labels = train_test_split(temp_files, temp_labels, test_size=0.667, \n",
    "                                                                  random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0fa8887-e451-4c31-8267-8e76cef664f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split training set for DataLoader\n",
    "num_splits = 4\n",
    "train_files_split = [train_files[i::num_splits] for i in range(num_splits)]\n",
    "train_labels_split = [train_labels[i::num_splits] for i in range(num_splits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a54c98d-478b-4381-abfb-6b425886693b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load validation sets\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    X_val = list(executor.map(process_video, val_files))\n",
    "\n",
    "X_val = torch.stack(X_val)#.to(device1)\n",
    "Y_val = torch.as_tensor(val_labels)#.to(device1)\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "val_dataset = TensorDataset(X_val, Y_val)\n",
    "del X_val, Y_val\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)  # Adjust batch_size as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaffcc72-b1ff-4583-b2df-7c5111fef858",
   "metadata": {
    "tags": []
   },
   "source": [
    "# loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bead41-faf1-4af7-96c3-aec5ef9d6f21",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = R3D_18_Weights.DEFAULT\n",
    "encoder_anchor = r3d_18(weights=weights)\n",
    "encoder_anchor = torch.nn.Sequential(*(list(encoder_anchor.children())[:-1]))\n",
    "encoder_anchor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "627c3a3a-ef51-4d47-9ff9-32a81c98ca72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['151610518_b.mp4',\n",
       " '151590941_b.mp4',\n",
       " '138350656.mp4',\n",
       " '131795642.mp4',\n",
       " '134035084_b.mp4',\n",
       " '142007427_b.mp4']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_files[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7af1adc3-e3fb-4cfc-b4e8-fec868a03a48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    example = list(executor.map(process_video, val_files[:6]))\n",
    "\n",
    "example = torch.stack(example)#.to(device1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67ff1951-9ea1-4092-8427-03175b18bb1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    features = encoder_anchor(example)\n",
    "    features = features.view(features.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f26569e1-6339-45fb-b03c-b14637c193aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 512])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "794acfae-eccf-4ce2-918b-58200b1ede84",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5386, 0.7802, 0.3420,  ..., 2.6961, 0.3758, 0.4522],\n",
       "        [1.3586, 0.3065, 0.9339,  ..., 1.0016, 0.6122, 0.3963],\n",
       "        [0.5657, 0.3217, 0.8942,  ..., 0.8140, 0.5058, 0.1867],\n",
       "        [1.0516, 0.9417, 0.6237,  ..., 1.2344, 1.2062, 0.5748],\n",
       "        [1.8407, 0.5805, 0.4934,  ..., 0.4045, 1.4958, 0.2209],\n",
       "        [1.0796, 0.4901, 0.9105,  ..., 2.0791, 1.2935, 0.4218]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e46e8c9-616f-4b48-be15-51066a53b821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "features = torch.nn.functional.normalize(features, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83a99b0e-8c53-46f6-9e76-20e6493ebf25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 512])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(7)\n",
    "# features = torch.rand(6, 512)\n",
    "features = features.unsqueeze(1)\n",
    "features.shape\n",
    "# features: [bsz, n_views, f_dim]\n",
    "# `n_views` is the number of crops from each image\n",
    "# better be L2 normalized in f_dim dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6fa301c-e61e-45c5-bfe2-897be507d61a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0213, 0.0309, 0.0136,  ..., 0.1068, 0.0149, 0.0179]],\n",
       "\n",
       "        [[0.0676, 0.0153, 0.0465,  ..., 0.0499, 0.0305, 0.0197]],\n",
       "\n",
       "        [[0.0390, 0.0222, 0.0616,  ..., 0.0561, 0.0349, 0.0129]],\n",
       "\n",
       "        [[0.0572, 0.0512, 0.0339,  ..., 0.0671, 0.0656, 0.0312]],\n",
       "\n",
       "        [[0.1043, 0.0329, 0.0279,  ..., 0.0229, 0.0847, 0.0125]],\n",
       "\n",
       "        [[0.0522, 0.0237, 0.0440,  ..., 0.1005, 0.0625, 0.0204]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5fae5d6-ff21-4073-b3ac-ed7f55d1d799",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = torch.as_tensor([1, 0, 2, 1, 2 ,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4812c9a-aab4-4a64-9a20-bad49724625d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = features.shape[0]\n",
    "labels = labels.contiguous().view(-1, 1)\n",
    "mask = torch.eq(labels, labels.T).float()#.to(device)\n",
    "\n",
    "contrast_count = features.shape[1]\n",
    "# Unbinds the features tensor along dimension 1 and concatenates the resulting tensors along dimension 0, resulting in a tensor of shape (5, 512).\n",
    "contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "\n",
    "anchor_feature = contrast_feature\n",
    "anchor_count = contrast_count\n",
    "\n",
    "temperature=10\n",
    "base_temperature=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f7cdd3b-2617-418e-99b7-bfba635008ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 1., 0., 1.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 1., 0.],\n",
       "        [1., 0., 0., 1., 0., 1.],\n",
       "        [0., 0., 1., 0., 1., 0.],\n",
       "        [1., 0., 0., 1., 0., 1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e85762ec-3286-4ccb-9c51-27fc6464857a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0213, 0.0309, 0.0136,  ..., 0.1068, 0.0149, 0.0179],\n",
       "        [0.0676, 0.0153, 0.0465,  ..., 0.0499, 0.0305, 0.0197],\n",
       "        [0.0390, 0.0222, 0.0616,  ..., 0.0561, 0.0349, 0.0129],\n",
       "        [0.0572, 0.0512, 0.0339,  ..., 0.0671, 0.0656, 0.0312],\n",
       "        [0.1043, 0.0329, 0.0279,  ..., 0.0229, 0.0847, 0.0125],\n",
       "        [0.0522, 0.0237, 0.0440,  ..., 0.1005, 0.0625, 0.0204]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16ddbbb7-2bac-40e3-991b-d96bade532f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48f43441-1191-4ba2-a216-6ab97512db11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 512])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrast_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d073b6a4-c52d-4996-a5c2-047521df4ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute logits\n",
    "anchor_dot_contrast = torch.div(\n",
    "    torch.matmul(anchor_feature, contrast_feature.T),\n",
    "    temperature)\n",
    "\n",
    "# for numerical stability\n",
    "logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "# tile mask\n",
    "#Repeats the mask tensor anchor_count times along the first dimension and contrast_count times along the second dimension.\n",
    "mask = mask.repeat(anchor_count, contrast_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8da2cd24-68fa-4752-9102-49521c689263",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.0788, 0.0748, 0.0772, 0.0759, 0.0881],\n",
       "        [0.0788, 0.1000, 0.0821, 0.0766, 0.0798, 0.0835],\n",
       "        [0.0748, 0.0821, 0.1000, 0.0806, 0.0872, 0.0820],\n",
       "        [0.0772, 0.0766, 0.0806, 0.1000, 0.0810, 0.0828],\n",
       "        [0.0759, 0.0798, 0.0872, 0.0810, 0.1000, 0.0805],\n",
       "        [0.0881, 0.0835, 0.0820, 0.0828, 0.0805, 0.1000]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_dot_contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82638a41-de82-4dab-9811-9bfcec58c55b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.0212, -0.0252, -0.0228, -0.0241, -0.0119],\n",
       "        [-0.0212,  0.0000, -0.0179, -0.0234, -0.0202, -0.0165],\n",
       "        [-0.0252, -0.0179,  0.0000, -0.0194, -0.0128, -0.0180],\n",
       "        [-0.0228, -0.0234, -0.0194,  0.0000, -0.0190, -0.0172],\n",
       "        [-0.0241, -0.0202, -0.0128, -0.0190,  0.0000, -0.0195],\n",
       "        [-0.0119, -0.0165, -0.0180, -0.0172, -0.0195,  0.0000]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c10fb97-d6c3-497a-b1fa-4351ff13839a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 1., 0., 1.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 1., 0.],\n",
       "        [1., 0., 0., 1., 0., 1.],\n",
       "        [0., 0., 1., 0., 1., 0.],\n",
       "        [1., 0., 0., 1., 0., 1.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70e019e4-91c8-4917-a245-fcbd406d88e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mask-out self-contrast cases, Creates a mask to zero out the diagonal elements (self-contrast cases) in the logits tensor.\n",
    "logits_mask = torch.scatter(\n",
    "    torch.ones_like(mask),\n",
    "    1,\n",
    "    torch.arange(batch_size * anchor_count).view(-1, 1),#.to(device),\n",
    "    0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c512d42-ee02-4439-a6fc-4be63468486f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 1., 1., 1.],\n",
       "        [1., 1., 1., 0., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 1.],\n",
       "        [1., 1., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae6ed73e-5ac0-46ae-9f7e-e40340f275cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask = mask * logits_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9625cfeb-6c7d-4e4c-a832-17ebdcd76e95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask\n",
    "# labels = torch.as_tensor([0, 1, 2, 2 ,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20bcc48e-38bb-4791-86ca-5211ba9a5a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute log_prob by exponentiating the logits, applying the mask, and normalizing.\n",
    "exp_logits = torch.exp(logits) * logits_mask\n",
    "log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28a45751-c391-4cf7-8ca8-3ad6f1bd8b54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.9790, 0.9751, 0.9774, 0.9762, 0.9882],\n",
       "        [0.9790, 0.0000, 0.9823, 0.9769, 0.9800, 0.9837],\n",
       "        [0.9751, 0.9823, 0.0000, 0.9808, 0.9873, 0.9821],\n",
       "        [0.9774, 0.9769, 0.9808, 0.0000, 0.9812, 0.9829],\n",
       "        [0.9762, 0.9800, 0.9873, 0.9812, 0.0000, 0.9807],\n",
       "        [0.9882, 0.9837, 0.9821, 0.9829, 0.9807, 0.0000]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be91f211-6841-42c7-a40e-6e7775201232",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.8959],\n",
       "        [4.9019],\n",
       "        [4.9077],\n",
       "        [4.8993],\n",
       "        [4.9055],\n",
       "        [4.9177]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_logits.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6717e1b7-bbd0-4789-a359-7c1027ff9b60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5884, -1.6096, -1.6136, -1.6112, -1.6125, -1.6003],\n",
       "        [-1.6109, -1.5896, -1.6075, -1.6130, -1.6098, -1.6061],\n",
       "        [-1.6160, -1.6087, -1.5908, -1.6102, -1.6036, -1.6088],\n",
       "        [-1.6119, -1.6125, -1.6084, -1.5891, -1.6081, -1.6063],\n",
       "        [-1.6144, -1.6105, -1.6031, -1.6093, -1.5903, -1.6098],\n",
       "        [-1.6047, -1.6093, -1.6108, -1.6101, -1.6123, -1.5928]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0af6e7cc-2751-4d25-a3d5-900ee5786543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute mean of log-likelihood over positive\n",
    "# modified to handle edge cases when there is no positive pair for an anchor point. \n",
    "# Edge case e.g.:- \n",
    "# features of shape: [4,1,...]\n",
    "# labels:            [0,1,1,2]\n",
    "# loss before mean:  [nan, ..., ..., nan] \n",
    "\n",
    "#Sums the mask along dimension 1 to count positive pairs and ensures no division by zero by replacing small values with 1.\n",
    "mask_pos_pairs = mask.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2df2e0d-8eb8-42a7-8083-cd0d55c195ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 0., 1., 2., 1., 2.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_pos_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7602606a-6577-4787-b131-99505823bd07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_pos_pairs = torch.where(mask_pos_pairs < 1e-6, 1, mask_pos_pairs)\n",
    "#mask_pos_pairs = torch.where(mask_pos_pairs < 1e-6, torch.tensor(1, dtype=mask_pos_pairs.dtype), mask_pos_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "81e52b56-3074-44c6-9a3b-098fb9ed806a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 1., 1., 2., 1., 2.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_pos_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5499e89-d526-4b43-afe3-688da75e120a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Computes the mean log probability of positive pairs.\n",
    "mean_log_prob_pos = (mask * log_prob).sum(1) / mask_pos_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6d6f7b79-f1d5-4afd-b3a8-7af871f83590",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000, -0.0000, -0.0000, -1.6112, -0.0000, -1.6003],\n",
       "        [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000],\n",
       "        [-0.0000, -0.0000, -0.0000, -0.0000, -1.6036, -0.0000],\n",
       "        [-1.6119, -0.0000, -0.0000, -0.0000, -0.0000, -1.6063],\n",
       "        [-0.0000, -0.0000, -1.6031, -0.0000, -0.0000, -0.0000],\n",
       "        [-1.6047, -0.0000, -0.0000, -1.6101, -0.0000, -0.0000]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask * log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5edd632c-736e-45d1-aec3-8eed013f0a39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.2115,  0.0000, -1.6036, -3.2182, -1.6031, -3.2148])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mask * log_prob).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "506af8a8-d7ae-4144-8481-5a4b538f0af6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6057,  0.0000, -1.6036, -1.6091, -1.6031, -1.6074])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_log_prob_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4fc941df-6993-48ad-af0b-50f00feac1d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Computes the loss by scaling the mean log probability and reshapes it to (anchor_count, batch_size).\n",
    "loss = - (temperature / base_temperature) * mean_log_prob_pos\n",
    "loss = loss.view(anchor_count, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fc61f961-ff8c-4405-b646-6d9be47c897c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6057, -0.0000, 1.6036, 1.6091, 1.6031, 1.6074]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be880065-4ec1-41bc-b8b7-ae84aec869bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Averages the loss over the batch.\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f357552f-e68c-485e-b475-c46e8fe4602d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3382)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed215ce7-9faf-4380-b2ad-1518cfb0a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=10.0, base_temperature=10.0): # temperature can be adjusted\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature))# Make temperature learnable\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        features = torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "        features = features.unsqueeze(1)\n",
    "        batch_size = features.shape[0]\n",
    "        labels = labels.contiguous().view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device1)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        # Unbinds the features tensor along dimension 1 and concatenates the resulting tensors along dimension 0.\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "\n",
    "        anchor_feature = contrast_feature\n",
    "        anchor_count = contrast_count\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(torch.matmul(anchor_feature, contrast_feature.T), self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device1), \n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mask_pos_pairs = mask.sum(1)\n",
    "        mask_pos_pairs = torch.where(mask_pos_pairs < 1e-6, 1, mask_pos_pairs)\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask_pos_pairs\n",
    "\n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6e072a-063c-4d09-a8d4-2c4957de05dd",
   "metadata": {},
   "source": [
    "# train encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7adcfa3f-57e6-43a2-8fb4-a09d19e018d0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BasicStem(\n",
       "    (0): Conv3d(3, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n",
       "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (5): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "encoder_anchor = encoder_anchor.to(device1)\n",
    "\n",
    "encoder_anchor.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "705c9533-dc9f-4f02-b89e-fc4f4eed2b02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "ContrastLoss = ContrastiveLoss(temperature=10.0, base_temperature=10.0)\n",
    "optimizer_encoder = optim.Adam(params = encoder_anchor.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fa7daa9-2177-45c9-aff0-f9e923362bed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_splits = 4\n",
    "train_datasets = []\n",
    "\n",
    "def process_split(split):\n",
    "    # load preprocessed video file\n",
    "    X_split = torch.load(f'Processed_video_r3d18/train/X_train_frame77_{split}.pt', weights_only=True)\n",
    "    Y_split = torch.load(f'Processed_video_r3d18/train/Y_train_frame77_{split}.pt', weights_only=True)\n",
    "    return X_split, Y_split\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = executor.map(process_split, range(num_splits))\n",
    "\n",
    "for X_split, Y_split in results:\n",
    "    train_datasets.append(TensorDataset(X_split, Y_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59238a80-2c23-47ea-a61a-b7840fd5018f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BatchSize = 64 \n",
    "train_loaders = []\n",
    "\n",
    "for split in range(num_splits):\n",
    "    train_loaders.append(DataLoader(dataset=train_datasets[split], batch_size=BatchSize, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a279b-6162-455c-96d8-0fc4f10f23a0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### start training ####\n",
    "best_val_ss = 0.0\n",
    "best_val_loss = 99999.9 \n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer_encoder, T_max=num_epochs, eta_min=0)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    results_file = open('results_encoder_ScratchLearn.txt', 'a') # create a txt file for saving the training records\n",
    "    running_loss = 0\n",
    "    for split in range(num_splits):\n",
    "        for data, targets in train_loaders[split]:\n",
    "            optimizer_encoder.zero_grad()\n",
    "            data = data.to(device1)\n",
    "            targets = targets.to(device1)\n",
    "\n",
    "            h = encoder_anchor(data)\n",
    "            h = h.view(h.size(0), -1)\n",
    "            contrastive_loss = ContrastLoss(h, targets)\n",
    "            running_loss += contrastive_loss\n",
    "\n",
    "            contrastive_loss.backward()\n",
    "            optimizer_encoder.step()\n",
    "        \n",
    "        del data, targets\n",
    "\n",
    "    # Validation phase\n",
    "    ss_result = []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_Y in val_loader:\n",
    "            batch_X = batch_X.to(device1)\n",
    "            batch_Y = batch_Y.to(device1)\n",
    "\n",
    "            h_val = encoder_anchor(batch_X)\n",
    "            h_val = h_val.view(h_val.size(0), -1)\n",
    "            val_loss += ContrastLoss(h_val, batch_Y)\n",
    "            ss = silhouette_score(h_val.cpu().numpy(), batch_Y.cpu().numpy())\n",
    "            ss_result.append(ss)\n",
    "\n",
    "\n",
    "    mean_ss = sum(ss_result) / len(ss_result)\n",
    "    results_file.write(f'Epoch [{epoch+1}/{num_epochs}], Contrastive Loss: {running_loss.item():.5f}, Val loss: {val_loss:.5f}, Val Silht score: {mean_ss:.5f}\\n')\n",
    "    \n",
    "    if mean_ss > best_val_ss:\n",
    "        best_val_ss = mean_ss\n",
    "        \n",
    "        torch.save(encoder_anchor.state_dict(), 'best_encoder_ScratchLearn.pth') # save the best video encoder\n",
    "        results_file.write(f'New best encoder saved with Silht scores: {best_val_ss}\\n')\n",
    "\n",
    "    del batch_X, batch_Y\n",
    "    torch.cuda.empty_cache()\n",
    "    results_file.close()\n",
    "    scheduler.step()\n",
    "    \n",
    "results_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5fe13ea-8ad3-4194-98ea-6bec43f01732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d5595-6f42-4dbe-9e3e-374e24e2df9a",
   "metadata": {},
   "source": [
    "# train classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6c4186-8121-413e-be3a-b8a26b095106",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = R3D_18_Weights.DEFAULT\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "trained_encoder = r3d_18()\n",
    "fc_layer = torch.nn.Linear(trained_encoder.fc.in_features, 3)\n",
    "\n",
    "trained_encoder = torch.nn.Sequential(*(list(trained_encoder.children())[:-1]))\n",
    "trained_encoder.load_state_dict(torch.load(\n",
    "    'best_encoder_ScratchLearn.pth',\n",
    "    weights_only = False,\n",
    "    map_location = device1))\n",
    "\n",
    "for param in trained_encoder.parameters():\n",
    "    param.requires_grad = False # don't require gradient for encoder\n",
    "    \n",
    "for param in fc_layer.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# eval mode\n",
    "trained_encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c1d12df3-4b01-4928-993d-d7fd9fcde249",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T23:31:47.284648Z",
     "iopub.status.busy": "2025-07-05T23:31:47.284117Z",
     "iopub.status.idle": "2025-07-05T23:31:47.287306Z",
     "shell.execute_reply": "2025-07-05T23:31:47.286893Z",
     "shell.execute_reply.started": "2025-07-05T23:31:47.284634Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([0.667, 0.854, 3.1], device=device1)\n",
    "CELoss = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')\n",
    "\n",
    "optimizer_fc = optim.Adam(fc_layer.parameters(), lr=1e-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f798d4e-1ae5-456f-b785-79b9f18865b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "trained_encoder = trained_encoder.to(device1)\n",
    "fc_layer = fc_layer.to(device1)\n",
    "\n",
    "fc_layer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3493a4ce-0142-4584-8b73-467621eeaeae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T18:28:12.223921Z",
     "iopub.status.busy": "2025-07-05T18:28:12.223643Z",
     "iopub.status.idle": "2025-07-05T18:28:12.236570Z",
     "shell.execute_reply": "2025-07-05T18:28:12.236171Z",
     "shell.execute_reply.started": "2025-07-05T18:28:12.223907Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('Output/SCL/train_X_encoder.pkl', 'rb') as f:  # load the learned representations from the encoder\n",
    "    X_train = pickle.load(f) \n",
    "\n",
    "with open('Output/SCL/train_Y_encoder.pkl', 'rb') as f:\n",
    "    Y_train = pickle.load(f)\n",
    "    \n",
    "X = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y = torch.tensor(Y_train, dtype=torch.long) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f61888eb-ea56-44ae-b185-e8495f55cc6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T23:27:07.358685Z",
     "iopub.status.busy": "2025-07-05T23:27:07.358169Z",
     "iopub.status.idle": "2025-07-05T23:27:07.360887Z",
     "shell.execute_reply": "2025-07-05T23:27:07.360560Z",
     "shell.execute_reply.started": "2025-07-05T23:27:07.358671Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BatchSize = 64\n",
    "\n",
    "train_dataset = TensorDataset(X, Y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BatchSize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5996eb0-3689-4ed2-a7df-1af14bb9f78b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-05T18:28:16.498533Z",
     "iopub.status.busy": "2025-07-05T18:28:16.498232Z",
     "iopub.status.idle": "2025-07-05T18:28:16.507141Z",
     "shell.execute_reply": "2025-07-05T18:28:16.506777Z",
     "shell.execute_reply.started": "2025-07-05T18:28:16.498521Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('Output/SCL/val_X_encoder.pkl', 'rb') as f: # load the learned representations from the encoder\n",
    "    val_X = pickle.load(f)\n",
    "\n",
    "with open('Output/SCL/val_Y_encoder.pkl', 'rb') as f:\n",
    "    val_Y = pickle.load(f)\n",
    "    \n",
    "val_X = torch.tensor(val_X, dtype=torch.float32)\n",
    "val_Y = torch.tensor(val_Y, dtype=torch.long) \n",
    "\n",
    "val_dataset = TensorDataset(val_X, val_Y)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98ec038-ced5-4d9d-9e53-8d56810afca2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_accuracy = 0.0\n",
    "best_val_loss = 99999.9\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer_fc, T_max=num_epochs, eta_min=0)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    results_file = open('results_SCL_fc_layer.txt', 'a') # create a txt file to save the training records\n",
    "    running_loss = 0\n",
    "\n",
    "    for data, targets in train_loader:\n",
    "        data = data.to(device1)\n",
    "        targets = targets.to(device1)\n",
    "        pred = fc_layer(data)\n",
    "        supervised_loss = CELoss(pred, targets)\n",
    "        optimizer_fc.zero_grad()\n",
    "        supervised_loss.backward()\n",
    "        optimizer_fc.step()\n",
    "        running_loss += supervised_loss\n",
    "    \n",
    "    # Validation phase\n",
    "    total_val_correct = 0\n",
    "    total_val = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_Y in test_loader:\n",
    "            batch_X = batch_X.to(device1)\n",
    "            batch_Y = batch_Y.to(device1)\n",
    "\n",
    "            val_scores = fc_layer(batch_X).softmax(-1) \n",
    "            _, val_predicted = torch.max(val_scores.data, 1)\n",
    "            total_val_correct += (val_predicted == batch_Y).sum().item()\n",
    "            total_val += batch_Y.size(0) \n",
    "            val_loss += CELoss(fc_layer(batch_X), batch_Y)\n",
    "\n",
    "    val_accuracy = total_val_correct / total_val \n",
    "    results_file.write(f'Epoch [{epoch+1}/{num_epochs}], Training loss: {running_loss.item():.4f}, Val loss: {val_loss:.4f}, Val accuracy: {val_accuracy:.3f}\\n')\n",
    "    \n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        torch.save(fc_layer.state_dict(), f'best_fc_layer.pth')\n",
    "        print(f'New best fc_layer saved with val accuracy: {val_accuracy}')\n",
    "        results_file.write(f'New best fc_layer saved with val accuracy: {val_accuracy}\\n')\n",
    "        \n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    results_file.close()\n",
    "    scheduler.step()\n",
    "\n",
    "results_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5d5406-136f-4ae8-9292-3f11471f01c4",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab811637-2344-4aa8-887a-b5ad864f2ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('Output/SCL/test_X_encoder.pkl', 'rb') as f: # load the learned representations from the encoder\n",
    "    X_test = pickle.load(f)\n",
    "\n",
    "with open('Output/SCL/test_Y_encoder.pkl', 'rb') as f:\n",
    "    Y_test = pickle.load(f)\n",
    "    \n",
    "test_X = torch.tensor(X_test, dtype=torch.float32)\n",
    "test_Y = torch.tensor(Y_test, dtype=torch.long) \n",
    "\n",
    "test_dataset = TensorDataset(test_X, test_Y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3778fa7a-d2db-4e99-af6f-dbee01e73735",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_encoder = r3d_18()\n",
    "fc_layer = torch.nn.Linear(trained_encoder.fc.in_features, 3)\n",
    "\n",
    "trained_encoder = torch.nn.Sequential(*(list(trained_encoder.children())[:-1]))\n",
    "trained_encoder.load_state_dict(torch.load(\n",
    "    'best_encoder_ScratchLearn.pth', weights_only=True,\n",
    "                                          map_location=device1\n",
    "                                          ))\n",
    "trained_encoder = trained_encoder.to(device1)\n",
    "trained_encoder.eval()\n",
    "\n",
    "\n",
    "fc_layer.load_state_dict(torch.load(\n",
    "    'best_fc_layer.pth', weights_only=True,\n",
    "                                   map_location=device1\n",
    "                                   ))\n",
    "fc_layer = fc_layer.to(device1)\n",
    "fc_layer.eval()\n",
    "type(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5201594-88fd-4fee-9761-add06d40d58f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_test_correct = 0\n",
    "total_test = 0\n",
    "pred_score = []\n",
    "pred_label = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_Y in test_loader:\n",
    "        batch_X = batch_X.to(device1)\n",
    "        batch_Y = batch_Y.to(device1)\n",
    "\n",
    "        test_scores = fc_layer(batch_X).softmax(-1)\n",
    "        _, test_predicted = torch.max(test_scores.data, 1)  # Get predictions\n",
    "        total_test_correct += (test_predicted == batch_Y).sum().item()  # Count correct predictions\n",
    "        total_test += batch_Y.size(0)  # Total predictions\n",
    "        pred_score.append(test_scores)\n",
    "        pred_label.append(test_predicted)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df98ce10-7d77-4626-9cc2-37cd3c8922c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unpack_tensors(tensor_list):\n",
    "    unpacked_tensors = []\n",
    "    for tensor in tensor_list:\n",
    "        for element in tensor:\n",
    "            unpacked_tensors.append(element.cpu().numpy())\n",
    "    return np.array(unpacked_tensors)\n",
    "\n",
    "y_score = unpack_tensors(pred_score)\n",
    "y_label = unpack_tensors(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4931976-6c52-45a3-9080-ba3553552ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute Accuracy\n",
    "accuracy = accuracy_score(np.array(Y_test), y_label)\n",
    "# Compute Confusion Matrix\n",
    "cm = confusion_matrix(np.array(Y_test), y_label)#test_y_true\n",
    "\n",
    "# Extract values for category y=1\n",
    "TP_y1 = cm[1, 1]  # True Positives for y=1\n",
    "FP_y1 = cm[0, 1] + cm[2, 1]  # False Positives for y=1\n",
    "FN_y1 = cm[1, 0] + cm[1, 2]  # False Negatives for y=1 \n",
    "# Calculate Precision and Recall for y=1\n",
    "precision_y1 = TP_y1 / (TP_y1 + FP_y1) if (TP_y1 + FP_y1) > 0 else 0\n",
    "recall_y1 = TP_y1 / (TP_y1 + FN_y1) if (TP_y1 + FN_y1) > 0 else 0\n",
    "\n",
    "# Extract values for category y=2\n",
    "TP_y2 = cm[2, 2]  # True Positives for y=2\n",
    "FP_y2 = cm[0, 2] + cm[1, 2]  # False Positives for y=2\n",
    "FN_y2 = cm[2, 0] + cm[2, 1]  # False Negatives for y=2\n",
    "# Calculate Precision and Recall for y=2\n",
    "precision_y2 = TP_y2 / (TP_y2 + FP_y2) if (TP_y2 + FP_y2) > 0 else 0\n",
    "recall_y2 = TP_y2 / (TP_y2 + FN_y2) if (TP_y2 + FN_y2) > 0 else 0\n",
    "\n",
    "# Compute F1 Score for Near-Crash\n",
    "f1_y1 = 2 * (precision_y1 * recall_y1) / (precision_y1 + recall_y1) if (precision_y1 + recall_y1) > 0 else 0\n",
    "# Compute F1 Score for Crash\n",
    "f1_y2 = 2 * (precision_y2 * recall_y2) / (precision_y2 + recall_y2) if (precision_y2 + recall_y2) > 0 else 0\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Recall for Crash, Near-Crash: {recall_y2:.3f}, {recall_y1:.3f}\")\n",
    "print(f\"Precision for Crash, Near-Crash: {precision_y2:.3f}, {precision_y1:.3f}\")\n",
    "print(f\"F1 Score for Crash, Near-Crash: {f1_y2:.3f}, {f1_y1:.3f}\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "62aa437b-7997-4a95-9d5a-00e9872270de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.133333333333334"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "77/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef06158a-1b78-4711-99de-9546e2308600",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define input transform\n",
    "side_size = 256 ## The size of the shorter side of the frame after scaling. \n",
    "mean = [0.45, 0.45, 0.45] # Mean & Standard deviation values used for normalization, one per color channel (RGB)\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256 ## The size to which the video frames will be center-cropped.\n",
    "\n",
    "# The number of frames to extract from the video.\n",
    "# The input video is temporally subsampled to this number of frames before being passed to the model.\n",
    "num_frames = 32\n",
    "\n",
    "sampling_rate = 4.7/2 # tune this!!!!! # The temporal stride of the fast pathway\n",
    "frames_per_second = 30/2 # Frames per second (FPS) for the video.\n",
    "\n",
    "slowfast_alpha = 4 # This parameter controls the relative speed between the slow and fast pathways.\n",
    "# For instance, `SLOWFAST_ALPHA = 4` means the slow pathway samples frames at a rate 4 times less frequent than the fast pathway, i.e., every 8 frames.\n",
    "\n",
    "# The duration of the input clip is also specific to the model.\n",
    "clip_duration = (num_frames * sampling_rate)/frames_per_second\n",
    "\n",
    "#num_clips = 10\n",
    "#num_crops = 3\n",
    "\n",
    "\n",
    "class PackPathway(torch.nn.Module):\n",
    "    \"\"\"Transform for converting video frames as a list of tensors\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, frames: torch.Tensor):\n",
    "        fast_pathway = frames\n",
    "        # Perform temporal sampling from the fast pathway.\n",
    "        slow_pathway = torch.index_select(\n",
    "            frames,\n",
    "            1,\n",
    "            torch.linspace(\n",
    "                0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha\n",
    "            ).long(),\n",
    "        )\n",
    "        frame_list = [slow_pathway, fast_pathway]\n",
    "        return frame_list\n",
    "\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size),\n",
    "            PackPathway()\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9eaded64-8dcd-4841-add6-f11201814282",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.013333333333334"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81c7f0b0-93fc-4976-9718-8de7b060270c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the duration of the clip to load by specifying the start and end duration\n",
    "# The start_sec should correspond to where the action occurs in the video\n",
    "start_sec = 0\n",
    "end_sec = start_sec + clip_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed20531c-27f3-4b4b-a73a-03d7547e8392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an EncodedVideo helper class and load the video\n",
    "video_path = path + train_files[2]\n",
    "video = EncodedVideo.from_path(video_path)\n",
    "# Load the desired clip\n",
    "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "\n",
    "# Apply a transform to normalize the video input\n",
    "video_data = transform(video_data)\n",
    "inputs = [i for i in video_data[\"video\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ffc4e93-348c-4c52-9f38-f1040b42a87c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([3, 8, 256, 256])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 32, 256, 256])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs))\n",
    "print(inputs[0].shape)\n",
    "print(type(inputs[0]))\n",
    "print(inputs[1].shape)\n",
    "print(type(inputs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ca7483ef-66d1-4221-a58c-741908c2e689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an EncodedVideo helper class and load the video\n",
    "video_path_1 = path + train_files[3]\n",
    "video_1 = EncodedVideo.from_path(video_path_1)\n",
    "# Load the desired clip\n",
    "video_data_1 = video_1.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "\n",
    "# Apply a transform to normalize the video input\n",
    "video_data_1 = transform(video_data_1)\n",
    "inputs_1 = [i for i in video_data_1[\"video\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "838c44d1-3d32-4eee-852e-1b8765e76f72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([3, 8, 256, 256])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 32, 256, 256])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs_1))\n",
    "print(inputs_1[0].shape)\n",
    "print(type(inputs_1[0]))\n",
    "print(inputs_1[1].shape)\n",
    "print(type(inputs_1[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "31053105-cf76-4d68-8e82-163ae08e6319",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tensor0 = torch.stack([inputs_1[0], inputs[0]])\n",
    "tensor1 = torch.stack([inputs_1[1], inputs[1]])\n",
    "inputs = [tensor0, tensor1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "94e48a6b-1d00-4229-bc71-0b1afcdc01a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs)\n",
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0bde6122-b2ac-4635-9324-1d4ccbedcfb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "26522686-2a32-4ec5-bf9a-e0ed86fde1a8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_video(file):\n",
    "    # Initialize an EncodedVideo helper class and load the video\n",
    "    video_path = path + file\n",
    "    try:\n",
    "        video = EncodedVideo.from_path(video_path)\n",
    "        # Load the desired clip\n",
    "        # {'video_data': Tensor of shape (T, C, H, W)} where T = number of frames, C = channels, H = height, W = width\n",
    "        video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "        # Apply a transform to normalize the video input\n",
    "        video_data = transform(video_data)\n",
    "\n",
    "        #prepare the input for the model\n",
    "        #inputs = [i[None, ...] for i in video_data[\"video\"]]\n",
    "        inputs = [i for i in video_data[\"video\"]]  # Shape: (1, T, C, H, W)\n",
    "        #inputs = torch.stack([i for i in video_data[\"video\"]])\n",
    "        return inputs\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"No video found at {video_path}\")\n",
    "        #video = None \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "tensor0 = []\n",
    "tensor1 = []\n",
    "'''        \n",
    "for file in train_files[:5]:\n",
    "    inputs = process_video(file)\n",
    "    tensor0.append(inputs[0])\n",
    "    tensor1.append(inputs[1])\n",
    "'''    \n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(process_video, train_files[:7]))\n",
    "\n",
    "tensor0 = torch.stack([inputs[0] for inputs in results])\n",
    "tensor1 = torch.stack([inputs[1] for inputs in results])\n",
    "\n",
    "example = [tensor0, tensor1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3013fbc6-4f3a-4e4d-8adc-780c4e622fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([7, 3, 8, 256, 256])\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([7, 3, 32, 256, 256])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(len(example))\n",
    "print(example[0].shape)\n",
    "print(type(example[0]))\n",
    "print(example[1].shape)\n",
    "print(type(example[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5d3a1376-7d7b-4a88-9e6b-77205efbb6e6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = model(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e6e8534f-db00-4593-8ddb-cfe9cf5fb76b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.7389e-03,  4.2112e-02, -5.6869e-02,  ..., -1.5899e-02,\n",
       "         -1.7572e-03,  4.5431e-02],\n",
       "        [ 9.0630e-03,  5.3385e-02, -6.8349e-02,  ..., -1.7457e-02,\n",
       "          2.2629e-05,  6.3389e-02],\n",
       "        [ 6.2081e-03,  4.5502e-02, -6.5099e-02,  ..., -2.5830e-02,\n",
       "          6.4489e-04,  3.1176e-02],\n",
       "        ...,\n",
       "        [ 1.2675e-02,  6.6793e-02, -8.1811e-02,  ..., -2.0216e-02,\n",
       "         -3.7418e-03,  7.7426e-02],\n",
       "        [ 1.4328e-02,  4.5447e-02, -5.6876e-02,  ..., -2.0222e-02,\n",
       "          1.0108e-03,  6.0961e-02],\n",
       "        [ 1.8613e-03,  4.8267e-02, -6.5689e-02,  ..., -2.2184e-02,\n",
       "          3.7392e-03,  3.8830e-02]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c8bad738-e15e-4b89-9eff-21bd76d39d47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = torch.nn.functional.normalize(pred, p=2, dim=1)\n",
    "torch.manual_seed(7)\n",
    "# features = torch.rand(6, 512)\n",
    "features = features.unsqueeze(1)\n",
    "features.shape\n",
    "# features: [bsz, n_views, f_dim]\n",
    "# `n_views` is the number of crops from each image\n",
    "# better be L2 normalized in f_dim dimension\n",
    "\n",
    "labels = torch.as_tensor([1, 0, 2, 1, 2 ,1, 0])\n",
    "batch_size = features.shape[0]\n",
    "labels = labels.contiguous().view(-1, 1)\n",
    "mask = torch.eq(labels, labels.T).float()#.to(device)\n",
    "\n",
    "contrast_count = features.shape[1]\n",
    "# Unbinds the features tensor along dimension 1 and concatenates the resulting tensors along dimension 0, resulting in a tensor of shape (5, 512).\n",
    "contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "\n",
    "anchor_feature = contrast_feature\n",
    "anchor_count = contrast_count\n",
    "\n",
    "temperature=10\n",
    "base_temperature=10\n",
    "\n",
    "# compute logits\n",
    "anchor_dot_contrast = torch.div(\n",
    "    torch.matmul(anchor_feature, contrast_feature.T),\n",
    "    temperature)\n",
    "\n",
    "# for numerical stability\n",
    "logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "# tile mask\n",
    "#Repeats the mask tensor anchor_count times along the first dimension and contrast_count times along the second dimension.\n",
    "mask = mask.repeat(anchor_count, contrast_count)\n",
    "\n",
    "# mask-out self-contrast cases, Creates a mask to zero out the diagonal elements (self-contrast cases) in the logits tensor.\n",
    "logits_mask = torch.scatter(\n",
    "    torch.ones_like(mask),\n",
    "    1,\n",
    "    torch.arange(batch_size * anchor_count).view(-1, 1),#.to(device),\n",
    "    0\n",
    ")\n",
    "\n",
    "mask = mask * logits_mask\n",
    "\n",
    "# compute log_prob by exponentiating the logits, applying the mask, and normalizing.\n",
    "exp_logits = torch.exp(logits) * logits_mask\n",
    "log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "mask_pos_pairs = mask.sum(1)\n",
    "mask_pos_pairs = torch.where(mask_pos_pairs < 1e-6, 1, mask_pos_pairs)\n",
    "#mask_pos_pairs = torch.where(mask_pos_pairs < 1e-6, torch.tensor(1, dtype=mask_pos_pairs.dtype), mask_pos_pairs)\n",
    "\n",
    "#Computes the mean log probability of positive pairs.\n",
    "mean_log_prob_pos = (mask * log_prob).sum(1) / mask_pos_pairs\n",
    "\n",
    "# Computes the loss by scaling the mean log probability and reshapes it to (anchor_count, batch_size).\n",
    "loss = - (temperature / base_temperature) * mean_log_prob_pos\n",
    "loss = loss.view(anchor_count, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5101f699-af06-48c4-9b24-a907bd466c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.0986, 0.0958, 0.0965, 0.0960, 0.0986, 0.0979],\n",
       "        [0.0986, 0.1000, 0.0927, 0.0944, 0.0990, 0.0965, 0.0985],\n",
       "        [0.0958, 0.0927, 0.1000, 0.0995, 0.0884, 0.0929, 0.0958],\n",
       "        [0.0965, 0.0944, 0.0995, 0.1000, 0.0904, 0.0931, 0.0975],\n",
       "        [0.0960, 0.0990, 0.0884, 0.0904, 0.1000, 0.0936, 0.0968],\n",
       "        [0.0986, 0.0965, 0.0929, 0.0931, 0.0936, 0.1000, 0.0945],\n",
       "        [0.0979, 0.0985, 0.0958, 0.0975, 0.0968, 0.0945, 0.1000]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_dot_contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc41e1c-d0bf-47ce-8809-25797272d1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c98e2004-8a0c-429a-9569-47da12632b19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7914, 1.7899, 1.7975, 1.7922, 1.7974, 1.7908, 1.7901]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a8a4aefc-cccf-475f-9c52-39be48bfa031",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7927, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a684b051-7810-462b-b2eb-35958131e555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#original_tensor = torch.randn(6, 512)\n",
    "tensor1 = torch.randn(6, 512)\n",
    "tensor2 = torch.randn(6, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "48b28cb4-1001-4a71-8171-0b7a4f995b98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7813, -1.8431,  0.0471,  ...,  1.2512, -0.2868, -1.3121],\n",
       "        [-0.6395, -1.1887,  0.1142,  ..., -1.1530, -0.4722,  0.9251],\n",
       "        [-2.4173,  0.6274,  0.9625,  ...,  1.5833, -0.6150,  1.4210],\n",
       "        [-0.4424, -1.1124,  0.3907,  ..., -0.4711,  0.3590, -0.4634],\n",
       "        [ 0.1884,  0.4230, -0.6830,  ...,  0.5008, -0.2486,  0.2541],\n",
       "        [ 1.1854, -1.0656,  0.1663,  ..., -1.1010,  0.3486,  0.8192]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "06dca843-bc78-4b2d-afe8-2c96890e3f57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7217,  0.3723,  1.0594,  ..., -0.8908, -0.6278,  0.4420],\n",
       "        [ 1.8054, -0.6130, -1.3816,  ..., -0.6997,  0.4745, -2.1738],\n",
       "        [ 0.1287, -0.4778,  1.9450,  ...,  0.3311, -0.7177, -0.6761],\n",
       "        [-1.9065,  0.7391, -0.5156,  ..., -0.0477, -0.5712, -0.1730],\n",
       "        [ 1.2424, -0.6269,  0.0848,  ..., -0.5125,  1.1627, -0.1825],\n",
       "        [-1.2697,  0.3114,  0.2778,  ..., -0.0835, -0.4085,  0.6941]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e1c45c09-f171-45c2-88d7-e460040b66b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tensor1, tensor2 = torch.split(original_tensor, int(original_tensor.shape[1]/2), dim=1)\n",
    "\n",
    "#tensor1 = torch.nn.functional.pad(tensor1, (0, int(original_tensor.shape[1]/2), 0, 0), mode='constant', value=0).unsqueeze(1)\n",
    "#tensor2 = torch.nn.functional.pad(tensor2, (int(original_tensor.shape[1]/2), 0, 0, 0), mode='constant', value=0).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1a94b2f0-0a9f-4914-9ab5-1254f15359f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.7813, -1.8431,  0.0471,  ...,  1.2512, -0.2868, -1.3121],\n",
       "         [-0.7217,  0.3723,  1.0594,  ..., -0.8908, -0.6278,  0.4420]],\n",
       "\n",
       "        [[-0.6395, -1.1887,  0.1142,  ..., -1.1530, -0.4722,  0.9251],\n",
       "         [ 1.8054, -0.6130, -1.3816,  ..., -0.6997,  0.4745, -2.1738]],\n",
       "\n",
       "        [[-2.4173,  0.6274,  0.9625,  ...,  1.5833, -0.6150,  1.4210],\n",
       "         [ 0.1287, -0.4778,  1.9450,  ...,  0.3311, -0.7177, -0.6761]],\n",
       "\n",
       "        [[-0.4424, -1.1124,  0.3907,  ..., -0.4711,  0.3590, -0.4634],\n",
       "         [-1.9065,  0.7391, -0.5156,  ..., -0.0477, -0.5712, -0.1730]],\n",
       "\n",
       "        [[ 0.1884,  0.4230, -0.6830,  ...,  0.5008, -0.2486,  0.2541],\n",
       "         [ 1.2424, -0.6269,  0.0848,  ..., -0.5125,  1.1627, -0.1825]],\n",
       "\n",
       "        [[ 1.1854, -1.0656,  0.1663,  ..., -1.1010,  0.3486,  0.8192],\n",
       "         [-1.2697,  0.3114,  0.2778,  ..., -0.0835, -0.4085,  0.6941]]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_tensor = torch.cat((tensor1.unsqueeze(1), tensor2.unsqueeze(1)), dim=1)\n",
    "res_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f8f86fea-ae5a-448b-affe-f28323c40c18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2, 512])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9ddd3df6-1053-4aa3-8743-b11c1d8d82c9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['151859367.mp4',\n",
       " '151600338.mp4',\n",
       " '151568557.mp4',\n",
       " '151569020_b.mp4',\n",
       " '151864990_b.mp4',\n",
       " '151574090_b.mp4']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = R3D_18_Weights.DEFAULT\n",
    "encoder_anchor = r3d_18(weights=weights)\n",
    "encoder_anchor = torch.nn.Sequential(*(list(encoder_anchor.children())[:-1]))\n",
    "encoder_anchor.eval()\n",
    "val_files[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "6894682c-91e1-4e19-90f5-f4cc619d3dad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocess = weights.transforms()\n",
    "\n",
    "def process_video0(file):\n",
    "    vid, _, _ = read_video(path + file, output_format=\"TCHW\", pts_unit='sec')\n",
    "    if vid.size(0) == 0:  # Check if the first dimension (time) is zero\n",
    "        return torch.ones(3, 76, 112, 112)  # Return a zero tensor with the specified shape\n",
    "    return preprocess(vid[:76])\n",
    "\n",
    "def process_video1(file):\n",
    "    vid, _, _ = read_video(path + file, output_format=\"TCHW\", pts_unit='sec')\n",
    "    if vid.size(0) == 0:  # Check if the first dimension (time) is zero\n",
    "        return torch.ones(3, 38, 112, 112)  # Return a zero tensor with the specified shape\n",
    "    return preprocess(vid[:38])\n",
    "\n",
    "def process_video2(file):\n",
    "    vid, _, _ = read_video(path + file, output_format=\"TCHW\", pts_unit='sec')\n",
    "    if vid.size(0) == 0:  # Check if the first dimension (time) is zero\n",
    "        return torch.ones(3, 38, 112, 112)  # Return a zero tensor with the specified shape\n",
    "    return preprocess(vid[38:76])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "66b5335b-40a7-49ee-ba12-963f6da38aa4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 76, 112, 112])\n",
      "torch.Size([6, 512])\n"
     ]
    }
   ],
   "source": [
    "with ThreadPoolExecutor() as executor:\n",
    "    example0 = list(executor.map(process_video0, val_files[:6]))\n",
    "example0 = torch.stack(example0)#.to(device1)\n",
    "print(example0.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    raw_features0 = encoder_anchor(example0)\n",
    "    raw_features0 = raw_features0.view(raw_features0.size(0), -1)\n",
    "print(raw_features0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ebf0e23b-d328-49f4-8249-a3a4845510fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    example1 = list(executor.map(process_video1, val_files[:6]))\n",
    "example1 = torch.stack(example1)#.to(device1)\n",
    "    \n",
    "with ThreadPoolExecutor() as executor:\n",
    "    example2 = list(executor.map(process_video2, val_files[:6]))\n",
    "example2 = torch.stack(example2)#.to(device1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    raw_features1 = encoder_anchor(example1)\n",
    "    raw_features1 = raw_features1.view(raw_features1.size(0), -1)\n",
    "    \n",
    "    raw_features2 = encoder_anchor(example2)\n",
    "    raw_features2 = raw_features2.view(raw_features2.size(0), -1)\n",
    "\n",
    "features = torch.cat((raw_features1.unsqueeze(1), raw_features2.unsqueeze(1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "57cdf3b7-bfd3-495b-8767-7860d46b68e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5803, 1.3810, 0.6058,  ..., 0.6929, 0.8700, 1.5157],\n",
       "        [0.9929, 0.3172, 1.2320,  ..., 0.1994, 1.0364, 0.3778],\n",
       "        [1.5709, 0.6001, 0.6412,  ..., 2.0286, 0.6618, 0.5057],\n",
       "        [0.8595, 0.4736, 0.6543,  ..., 0.9596, 1.8751, 0.1583],\n",
       "        [0.3284, 0.7914, 1.0755,  ..., 1.3423, 0.2285, 0.0482],\n",
       "        [1.0450, 0.2499, 0.5111,  ..., 1.2609, 0.8196, 0.0361]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_features1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e1b91177-7030-4a72-bb02-5d2d7e4ac747",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6389, 1.4686, 0.4565,  ..., 0.6705, 0.8350, 0.9534],\n",
       "        [1.2422, 0.1993, 1.1996,  ..., 0.3300, 0.8838, 0.3698],\n",
       "        [1.3286, 0.4192, 0.7180,  ..., 1.7197, 0.5852, 0.8142],\n",
       "        [1.1361, 0.4766, 0.4306,  ..., 1.1108, 1.4538, 0.2129],\n",
       "        [0.3378, 0.4026, 1.7973,  ..., 1.4295, 0.4509, 0.0473],\n",
       "        [0.6738, 0.2376, 0.5889,  ..., 1.0103, 1.0471, 0.0174]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "961a83c2-ab35-400c-aec4-40ab87914270",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5803, 1.3810, 0.6058,  ..., 0.6929, 0.8700, 1.5157],\n",
       "         [0.6389, 1.4686, 0.4565,  ..., 0.6705, 0.8350, 0.9534]],\n",
       "\n",
       "        [[0.9929, 0.3172, 1.2320,  ..., 0.1994, 1.0364, 0.3778],\n",
       "         [1.2422, 0.1993, 1.1996,  ..., 0.3300, 0.8838, 0.3698]],\n",
       "\n",
       "        [[1.5709, 0.6001, 0.6412,  ..., 2.0286, 0.6618, 0.5057],\n",
       "         [1.3286, 0.4192, 0.7180,  ..., 1.7197, 0.5852, 0.8142]],\n",
       "\n",
       "        [[0.8595, 0.4736, 0.6543,  ..., 0.9596, 1.8751, 0.1583],\n",
       "         [1.1361, 0.4766, 0.4306,  ..., 1.1108, 1.4538, 0.2129]],\n",
       "\n",
       "        [[0.3284, 0.7914, 1.0755,  ..., 1.3423, 0.2285, 0.0482],\n",
       "         [0.3378, 0.4026, 1.7973,  ..., 1.4295, 0.4509, 0.0473]],\n",
       "\n",
       "        [[1.0450, 0.2499, 0.5111,  ..., 1.2609, 0.8196, 0.0361],\n",
       "         [0.6738, 0.2376, 0.5889,  ..., 1.0103, 1.0471, 0.0174]]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b0df1c9c-1826-4470-82bc-06d05461f787",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2, 512])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "d496d4ce-25ca-43ab-a84f-fa411b088a64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ys = torch.randn(6)\n",
    "\n",
    "fakedataset = TensorDataset(features, ys)\n",
    "fakeloader = DataLoader(fakedataset, batch_size=10, shuffle=False)  # Adjust batch_size as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9114f66-fe81-4d76-bc65-0e7354414d1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#raw_features = torch.nn.functional.normalize(raw_features, p=2, dim=2)\n",
    "#features = features.unsqueeze(1)\n",
    "#raw_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03d335c-6ac7-4340-b1a8-62146e3d7543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tensor1, tensor2 = torch.split(raw_features, int(raw_features.shape[1]/2), dim=1)\n",
    "\n",
    "#tensor1 = torch.nn.functional.pad(tensor1, (0, int(raw_features.shape[1]/2), 0, 0), mode='constant', value=0).unsqueeze(1)\n",
    "#tensor2 = torch.nn.functional.pad(tensor2, (int(raw_features.shape[1]/2), 0, 0, 0), mode='constant', value=0).unsqueeze(1)\n",
    "#features = torch.cat((tensor1, tensor2), dim=1)\n",
    "#features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "f20bdaaf-ec45-4bf5-8d2a-1c1cbadb33ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = features.shape[0]\n",
    "mask = torch.eye(batch_size, dtype=torch.float32)\n",
    "\n",
    "contrast_count = features.shape[1]\n",
    "# Unbinds the features tensor along dimension 1 and concatenates the resulting tensors along dimension 0, resulting in a tensor of shape (5, 512).\n",
    "contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "\n",
    "anchor_feature = contrast_feature\n",
    "anchor_count = contrast_count\n",
    "\n",
    "temperature=10\n",
    "base_temperature=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a07934a4-0418-4ace-a273-ae2a7cb79215",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ca27ebc9-2177-4c68-b18b-9abe3a448941",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrast_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b9539bec-73a8-4ad0-bcb1-d4b072531f78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "83a9e14d-2436-40a2-9641-36dce80d069c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute logits\n",
    "anchor_dot_contrast = torch.div(\n",
    "    torch.matmul(anchor_feature, contrast_feature.T),\n",
    "    temperature)\n",
    "\n",
    "# for numerical stability\n",
    "logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "# tile mask\n",
    "#Repeats the mask tensor anchor_count times along the first dimension and contrast_count times along the second dimension.\n",
    "mask = mask.repeat(anchor_count, contrast_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d3692cfa-8ea4-44a2-b3d1-63bf147b9ef8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.0000,  -7.4198,  -2.5497,  -7.3351, -11.8793,  -6.4295,  -1.9893,\n",
       "          -7.3531,  -2.0151,  -6.6593, -10.8924,  -8.3623],\n",
       "        [ -9.8684,   0.0000,  -5.6126,  -5.0045, -12.0003,  -6.0410, -10.7717,\n",
       "          -0.5895,  -6.1009,  -5.8015, -11.4308,  -7.8125],\n",
       "        [-13.1465, -13.7609,   0.0000, -11.4468, -19.6793, -11.5500, -14.6907,\n",
       "         -14.1504,  -2.2684, -11.2548, -17.7309, -13.5254],\n",
       "        [ -9.4456,  -4.6665,  -2.9605,   0.0000, -10.6302,  -4.5949, -10.2319,\n",
       "          -4.2290,  -4.3877,  -2.3372,  -9.9660,  -6.6995],\n",
       "        [ -3.7758,  -1.4482,  -0.9789,  -0.4161,   0.0000,  -2.0873,  -4.1295,\n",
       "          -1.4917,  -0.8156,  -1.0051,  -1.8562,  -3.8388],\n",
       "        [ -8.7188,  -5.8817,  -3.2425,  -4.7737, -12.4802,   0.0000,  -9.6851,\n",
       "          -5.9796,  -4.9169,  -5.0875, -12.1852,  -2.9672],\n",
       "        [ -1.4291,  -7.7629,  -3.5336,  -7.5611, -11.6728,  -6.8355,   0.0000,\n",
       "          -7.8035,  -2.6586,  -6.7828, -10.7477,  -8.8981],\n",
       "        [-11.4042,  -2.1919,  -7.6046,  -6.1695, -13.6463,  -7.7413, -12.4147,\n",
       "           0.0000,  -8.4264,  -6.9463, -12.5328,  -9.7164],\n",
       "        [-13.8508, -15.4881,  -3.5072, -14.1129, -20.7549, -14.4632, -15.0545,\n",
       "         -16.2110,   0.0000, -13.5520, -18.4064, -16.4377],\n",
       "        [ -7.6092,  -4.3028,  -1.6078,  -1.1765, -10.0585,  -3.7480,  -8.2928,\n",
       "          -3.8451,  -2.6661,   0.0000,  -9.5973,  -6.1309],\n",
       "        [ -5.6427,  -3.7326,  -1.8844,  -2.6058,  -4.7101,  -4.6462,  -6.0582,\n",
       "          -3.2321,  -1.3211,  -3.3978,   0.0000,  -6.0886],\n",
       "        [ -7.6844,  -4.6861,  -2.2507,  -3.9111, -11.2645,   0.0000,  -8.7804,\n",
       "          -4.9875,  -3.9242,  -4.5032, -10.6604,  -1.2776]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "51be8c6a-cd67-40d9-a79a-277256c1e66c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "5b4fea4b-28f0-44f5-8828-7086cb26b111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mask-out self-contrast cases, Creates a mask to zero out the diagonal elements (self-contrast cases) in the logits tensor.\n",
    "logits_mask = torch.scatter(\n",
    "    torch.ones_like(mask),\n",
    "    1,\n",
    "    torch.arange(batch_size * anchor_count).view(-1, 1),#.to(device),\n",
    "    0)\n",
    "mask = mask * logits_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5bea20ab-a4ab-4231-8b84-81304587160c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9efbbb21-5b40-4e1f-a58d-f97309e944c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute log_prob by exponentiating the logits, applying the mask, and normalizing.\n",
    "exp_logits = torch.exp(logits) * logits_mask\n",
    "log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "mask_pos_pairs = mask.sum(1)\n",
    "mask_pos_pairs = torch.where(mask_pos_pairs < 1e-6, 1, mask_pos_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b9085419-74e2-4007-84a7-a4d50eca5e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_log_prob_pos = (mask * log_prob).sum(1) / mask_pos_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8f95eea6-5a66-4a7f-8ea0-479340371706",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = - (temperature / base_temperature) * mean_log_prob_pos\n",
    "loss = loss.view(anchor_count, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "7c4a701d-e539-45af-9e53-87d67e059cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f78647f8-3800-443e-9bf3-13520292b203",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9051)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmaction",
   "language": "python",
   "name": "mmaction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
